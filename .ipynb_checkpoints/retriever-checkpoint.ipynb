{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWpbp0N9fkPX",
    "outputId": "49b2c831-838f-4adc-b7d7-ccbbd165139d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/cs231n/final project\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "DIRECTORY = \"/content/drive/My Drive/cs231n/final project\"\n",
    "%cd $DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFy0pdwMfZ-9",
    "outputId": "44cb5822-e833-41f9-efc1-ec56aefa2a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "wVssOaGbDPUI"
   },
   "outputs": [],
   "source": [
    "imageEmbeddingSize, queryTextEmbeddingSize = 512, 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "BWelGQatLZ7y"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# Load in data and unwrap it\n",
    "# Function to convert stringified tuple keys back to tuples\n",
    "def unwrap_keys(mapping):\n",
    "    return {literal_eval(k): v for k, v in mapping.items()}\n",
    "\n",
    "# Load the JSON file\n",
    "with open('data_augmentation/embeddings/clip_embeddings.json', 'r') as json_file:\n",
    "    data_from_json = json.load(json_file)\n",
    "\n",
    "# print(data_from_json)\n",
    "# Unwrap the keys to their original tuple format\n",
    "unwrapped_data = unwrap_keys(data_from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_A5Lsxeq6-w",
    "outputId": "25cccd8e-c9e5-462d-eb29-826e6cb77fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224 224\n"
     ]
    }
   ],
   "source": [
    "text_key = \"text_embedding\"\n",
    "image_key = \"image_embeddings_all\"\n",
    "score_key = \"scores\"\n",
    "image_embeddings_top_5 = \"image_embeddings_top5_idx\"\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "y_output = []\n",
    "X_image_eval = []\n",
    "X_text_embed_eval = []\n",
    "y_eval = []\n",
    "prompts = []\n",
    "for key, sub_dataset in unwrapped_data.items():\n",
    "  # pprint.pp(sub_dataset)\n",
    "  text_embedding = sub_dataset[text_key]\n",
    "  image_embedding = sub_dataset[image_key]\n",
    "  # print(len(image_embedding))\n",
    "  scores = sub_dataset[score_key]\n",
    "  top5 = sub_dataset[image_embeddings_top_5]\n",
    "  # print(len(image_embedding), len(text_embedding), len(scores))\n",
    "  # if len(image_embedding) != len(scores):\n",
    "  #   continue\n",
    "  # print(top5, len(image_embedding))\n",
    "  # print(key)\n",
    "  for i in range(1):\n",
    "    idx = top5[i]\n",
    "    # print(top5, len(image_embedding))\n",
    "    if idx >= len(image_embedding):\n",
    "      # print(top5, len(image_embedding))\n",
    "      print(key)\n",
    "      continue\n",
    "    image_embeddings.append(image_embedding[idx])\n",
    "    text_embeddings.append(text_embedding)\n",
    "    y_output.append(scores[i])\n",
    "  if top5[0] == 0:\n",
    "    continue\n",
    "  X_image_eval.append(image_embedding)\n",
    "  X_text_embed_eval.append([text_embedding] * len(image_embedding))\n",
    "  y_eval.append(top5)\n",
    "  prompts.append(key)\n",
    "\n",
    "# print(y_output)\n",
    "N = len(image_embeddings)\n",
    "print(len(image_embeddings), len(text_embeddings), len(y_output))\n",
    "# print(y_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJI9LC23qRFj",
    "outputId": "4777a836-1832-4214-cb75-7c5fec75e307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 512]) torch.Size([224, 384]) torch.Size([224])\n"
     ]
    }
   ],
   "source": [
    "X_images = torch.tensor(image_embeddings).squeeze()\n",
    "X_queries = torch.tensor(text_embeddings)\n",
    "y = torch.tensor(y_output)\n",
    "print(X_images.shape, X_queries.shape, y.shape)\n",
    "dataset = TensorDataset(X_images, X_queries, y)\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# print(X_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "kvJJm9zsf2BO"
   },
   "outputs": [],
   "source": [
    "class EmbeddingProjectionNN(nn.Module):\n",
    "    def __init__(self, embeddingSize):\n",
    "      super().__init__()\n",
    "      self.linear_relu_stack = nn.Sequential(\n",
    "          nn.Linear(embeddingSize, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(),\n",
    "          nn.Linear(512, 512),\n",
    "          nn.LayerNorm(512),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(),\n",
    "          nn.Linear(512, 256),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      # input (N, E)\n",
    "      projection = self.linear_relu_stack(x)\n",
    "      return projection\n",
    "\n",
    "class RetrieverNN(nn.Module):\n",
    "  def __init__(self, imageEmbedding, queryEmbedding):\n",
    "    super().__init__()\n",
    "    self.imageProj = EmbeddingProjectionNN(imageEmbedding)\n",
    "    self.queryProj = EmbeddingProjectionNN(queryEmbedding)\n",
    "    self.similarity = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "\n",
    "  def forward(self, images, query):\n",
    "    imageProj = self.imageProj(images)\n",
    "    queryProj = self.queryProj(query)\n",
    "    similarities = self.similarity(imageProj, queryProj)\n",
    "    # make similarities a prob scores between 0 and 1\n",
    "    scores = similarities * 0.5 + 0.5\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "3thaF3qkk8aW"
   },
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "def train_part34(model, optimizer, epochs=10, scheduler = None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for imageEmbeddings, queryEmbedding, y in dataloader:\n",
    "          # (B, E1), (B, E2), (B,)\n",
    "          model.train()  # put model to training mode\n",
    "          imageEmbeddings = imageEmbeddings.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "          queryEmbedding = queryEmbedding.to(device=device, dtype=dtype)\n",
    "          y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "          scores = model(imageEmbeddings, queryEmbedding)\n",
    "          # print(scores)\n",
    "          # print(y)\n",
    "          output = loss(scores, y)\n",
    "          # print(scores, y)\n",
    "\n",
    "          # Zero out all of the gradients for the variables which the optimizer\n",
    "          # will update.\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # This is the backwards pass: compute the gradient of the loss with\n",
    "          # respect to each  parameter of the model.\n",
    "          output.backward()\n",
    "\n",
    "          # Actually update the parameters of the model using the gradients\n",
    "          # computed by the backwards pass.\n",
    "          optimizer.step()\n",
    "          if scheduler:\n",
    "            scheduler.step()\n",
    "          print('Iteration %d, loss = %.4f' % (e, output.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVxih4THo1JC",
    "outputId": "15f5c063-321a-4aeb-b05b-daf626540a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.7039\n",
      "Iteration 0, loss = 0.7194\n",
      "Iteration 0, loss = 0.8504\n",
      "Iteration 0, loss = 0.6669\n",
      "Iteration 0, loss = 0.9827\n",
      "Iteration 0, loss = 0.6768\n",
      "Iteration 0, loss = 0.7512\n",
      "Iteration 1, loss = 0.6793\n",
      "Iteration 1, loss = 0.6346\n",
      "Iteration 1, loss = 0.6451\n",
      "Iteration 1, loss = 0.6103\n",
      "Iteration 1, loss = 0.5618\n",
      "Iteration 1, loss = 0.5766\n",
      "Iteration 1, loss = 0.5602\n",
      "Iteration 2, loss = 0.4614\n",
      "Iteration 2, loss = 0.5956\n",
      "Iteration 2, loss = 0.5109\n",
      "Iteration 2, loss = 0.5378\n",
      "Iteration 2, loss = 0.5843\n",
      "Iteration 2, loss = 0.5393\n",
      "Iteration 2, loss = 0.5371\n",
      "Iteration 3, loss = 0.5448\n",
      "Iteration 3, loss = 0.5375\n",
      "Iteration 3, loss = 0.5944\n",
      "Iteration 3, loss = 0.5614\n",
      "Iteration 3, loss = 0.4577\n",
      "Iteration 3, loss = 0.4617\n",
      "Iteration 3, loss = 0.4943\n",
      "Iteration 4, loss = 0.5357\n",
      "Iteration 4, loss = 0.4877\n",
      "Iteration 4, loss = 0.5172\n",
      "Iteration 4, loss = 0.4461\n",
      "Iteration 4, loss = 0.5394\n",
      "Iteration 4, loss = 0.5378\n",
      "Iteration 4, loss = 0.5534\n",
      "Iteration 5, loss = 0.5018\n",
      "Iteration 5, loss = 0.5575\n",
      "Iteration 5, loss = 0.5249\n",
      "Iteration 5, loss = 0.4128\n",
      "Iteration 5, loss = 0.5397\n",
      "Iteration 5, loss = 0.4382\n",
      "Iteration 5, loss = 0.6222\n",
      "Iteration 6, loss = 0.4915\n",
      "Iteration 6, loss = 0.5463\n",
      "Iteration 6, loss = 0.4915\n",
      "Iteration 6, loss = 0.4682\n",
      "Iteration 6, loss = 0.4748\n",
      "Iteration 6, loss = 0.4873\n",
      "Iteration 6, loss = 0.5817\n",
      "Iteration 7, loss = 0.4128\n",
      "Iteration 7, loss = 0.4708\n",
      "Iteration 7, loss = 0.4959\n",
      "Iteration 7, loss = 0.5653\n",
      "Iteration 7, loss = 0.5013\n",
      "Iteration 7, loss = 0.4710\n",
      "Iteration 7, loss = 0.4754\n",
      "Iteration 8, loss = 0.5463\n",
      "Iteration 8, loss = 0.5343\n",
      "Iteration 8, loss = 0.3650\n",
      "Iteration 8, loss = 0.4189\n",
      "Iteration 8, loss = 0.5541\n",
      "Iteration 8, loss = 0.3890\n",
      "Iteration 8, loss = 0.3459\n",
      "Iteration 9, loss = 0.4731\n",
      "Iteration 9, loss = 0.4138\n",
      "Iteration 9, loss = 0.3444\n",
      "Iteration 9, loss = 0.3205\n",
      "Iteration 9, loss = 0.5048\n",
      "Iteration 9, loss = 0.3692\n",
      "Iteration 9, loss = 0.3066\n",
      "Iteration 10, loss = 0.4485\n",
      "Iteration 10, loss = 0.3379\n",
      "Iteration 10, loss = 0.2847\n",
      "Iteration 10, loss = 0.3554\n",
      "Iteration 10, loss = 0.6126\n",
      "Iteration 10, loss = 0.3106\n",
      "Iteration 10, loss = 0.2304\n",
      "Iteration 11, loss = 0.5318\n",
      "Iteration 11, loss = 0.2670\n",
      "Iteration 11, loss = 0.3881\n",
      "Iteration 11, loss = 0.3614\n",
      "Iteration 11, loss = 0.3247\n",
      "Iteration 11, loss = 0.3806\n",
      "Iteration 11, loss = 0.4161\n",
      "Iteration 12, loss = 0.3538\n",
      "Iteration 12, loss = 0.4092\n",
      "Iteration 12, loss = 0.2480\n",
      "Iteration 12, loss = 0.2951\n",
      "Iteration 12, loss = 0.2760\n",
      "Iteration 12, loss = 0.3131\n",
      "Iteration 12, loss = 0.2745\n",
      "Iteration 13, loss = 0.3707\n",
      "Iteration 13, loss = 0.2679\n",
      "Iteration 13, loss = 0.3291\n",
      "Iteration 13, loss = 0.4014\n",
      "Iteration 13, loss = 0.2938\n",
      "Iteration 13, loss = 0.3230\n",
      "Iteration 13, loss = 0.2578\n",
      "Iteration 14, loss = 0.3563\n",
      "Iteration 14, loss = 0.1694\n",
      "Iteration 14, loss = 0.2885\n",
      "Iteration 14, loss = 0.3532\n",
      "Iteration 14, loss = 0.3799\n",
      "Iteration 14, loss = 0.3167\n",
      "Iteration 14, loss = 0.1401\n",
      "Iteration 15, loss = 0.2093\n",
      "Iteration 15, loss = 0.3093\n",
      "Iteration 15, loss = 0.1278\n",
      "Iteration 15, loss = 0.2693\n",
      "Iteration 15, loss = 0.4233\n",
      "Iteration 15, loss = 0.2825\n",
      "Iteration 15, loss = 0.3697\n",
      "Iteration 16, loss = 0.0670\n",
      "Iteration 16, loss = 0.2217\n",
      "Iteration 16, loss = 0.2536\n",
      "Iteration 16, loss = 0.2430\n",
      "Iteration 16, loss = 0.1501\n",
      "Iteration 16, loss = 0.4282\n",
      "Iteration 16, loss = 0.6355\n",
      "Iteration 17, loss = 0.3407\n",
      "Iteration 17, loss = 0.1719\n",
      "Iteration 17, loss = 0.4218\n",
      "Iteration 17, loss = 0.1912\n",
      "Iteration 17, loss = 0.3732\n",
      "Iteration 17, loss = 0.4000\n",
      "Iteration 17, loss = 0.3062\n",
      "Iteration 18, loss = 0.1868\n",
      "Iteration 18, loss = 0.2366\n",
      "Iteration 18, loss = 0.3018\n",
      "Iteration 18, loss = 0.4129\n",
      "Iteration 18, loss = 0.2901\n",
      "Iteration 18, loss = 0.4813\n",
      "Iteration 18, loss = 0.2326\n",
      "Iteration 19, loss = 0.2473\n",
      "Iteration 19, loss = 0.2727\n",
      "Iteration 19, loss = 0.2732\n",
      "Iteration 19, loss = 0.2280\n",
      "Iteration 19, loss = 0.1889\n",
      "Iteration 19, loss = 0.1659\n",
      "Iteration 19, loss = 0.4847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RetrieverNN(imageEmbeddingSize, queryTextEmbeddingSize)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "train_part34(model, optimizer, epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuDiH8VwN-_y",
    "outputId": "ec0ae1bf-01b7-4e4f-96f9-0f657ec4962d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(24,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "(12,)\n",
      "0.135 0.37\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "eval_loose = 0\n",
    "eval_tight = 0\n",
    "N = len(X_image_eval)\n",
    "with torch.no_grad():\n",
    "  for i in range(N):\n",
    "    X_image = torch.tensor(X_image_eval[i]).squeeze()\n",
    "    # print(X_image.shape)\n",
    "    X_query_eval = torch.tensor(X_text_embed_eval[i])\n",
    "    # print(X_query_eval.shape)\n",
    "    top5 = y_eval[i]\n",
    "    probs = model(X_image, X_query_eval)\n",
    "    top_pred = probs.detach().numpy().squeeze()\n",
    "    # print(prompts[i])\n",
    "    print(top_pred.shape)\n",
    "    pred_idx = np.argmax(top_pred)\n",
    "    if pred_idx in top5:\n",
    "      eval_loose += 1\n",
    "    if pred_idx == top5[0]:\n",
    "      eval_tight += 1\n",
    "print(float(eval_tight)/ N, float(eval_loose)/N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQnIByP_tObl"
   },
   "source": [
    "## Connect to Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "[1 2 6 3 8]\n",
      "[1, 1, 1, 1, 1]\n",
      "Successfully retrieved image embeddings\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\"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\")\n",
    "# Simulated model output\n",
    "# probs = model(X_image, X_query_eval)\n",
    "# top_pred = probs.detach().numpy().squeeze()\n",
    "# For the sake of this example, let's simulate top_pred\n",
    "X_image = torch.tensor(X_image_eval[0]).squeeze()\n",
    "X_query_eval = torch.tensor(X_text_embed_eval[0])\n",
    "probs = model(X_image, X_query_eval)\n",
    "top_pred = probs.detach().numpy().squeeze()\n",
    "print(top_pred.shape)\n",
    "top_5_indices = np.argsort(top_pred)[-5:][::-1]\n",
    "\n",
    "print(top_5_indices)\n",
    "prompt = ('what is the Alex desk', 1)\n",
    "# Retrieve the image embeddings corresponding to the top predictions\n",
    "retrieved_image_embeddings = [unwrapped_data[prompt][\"image_embeddings_all\"][idx] for idx in top_5_indices]\n",
    "print(([len(x) for x in retrieved_image_embeddings]))\n",
    "print(\"Successfully retrieved image embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt, retrieved_image_embeddings):\n",
    "    full_prompt =f\"\"\"Given a target sentence and a set of image embeddings, representing relevant images related to the sentence, answer the question in the sentence.\n",
    "### Target sentence:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "### Image embeddings:\n",
    "# {str(retrieved_image_embeddings[:2])}\n",
    "# \"\"\"\n",
    "    # print(full_prompt)\n",
    "    return tokenizer(full_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8beb957c077440ba91cc7cd7ff5e6d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Generator and Tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Load the generator model (GPT-Neo in this example) and tokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"  # You can choose a different model from Hugging Face\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(model_name, token=\"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\",resume_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\",resume_download=True)\n",
    "print(\"Loaded Generator and Tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=generator_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_ids))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate the output using the context\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m generator_model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Decode the generated output\u001b[39;00m\n\u001b[1;32m      9\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1562\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1560\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1561\u001b[0m )\n\u001b[0;32m-> 1562\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1564\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the context input for the generator model\n",
    "# Tokenize the input context\n",
    "input_ids = generate_and_tokenize_prompt(prompt, retrieved_image_embeddings)\n",
    "print(len(input_ids))\n",
    "# Generate the output using the context\n",
    "output = generator_model.generate(input_ids, max_length=50)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
