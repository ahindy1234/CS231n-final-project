{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f2aa02-e524-4272-9b55-d702f742def7",
   "metadata": {},
   "source": [
    "# Script Description\n",
    "\n",
    "This script performs the following tasks:\n",
    "\n",
    "1. **Load Libraries**:\n",
    "   - Uses `json` for handling JSON files.\n",
    "   - Uses `sentence-transformers` for generating text embeddings.\n",
    "   - Uses `torch` and `torchvision` for generating image embeddings.\n",
    "   - Uses `PIL` for image processing.\n",
    "   - Uses `os` for file path operations.\n",
    "\n",
    "2. **Load Data**:\n",
    "   - Loads `initial_dataset_no_aug.json` (or the relevant json) which contains the main dataset.\n",
    "   - Loads `image_metadata.json` which contains metadata about images.\n",
    "\n",
    "3. **Initialize Models**:\n",
    "   - Initializes a text embedding model (`all-MiniLM-L6-v2`) from `sentence-transformers`.\n",
    "   - Loads a pre-trained ResNet-18 model from `torchvision` for image embeddings.\n",
    "\n",
    "4. **Preprocess Images**:\n",
    "   - Defines a preprocessing pipeline to resize, crop, convert to tensor, and normalize images.\n",
    "\n",
    "5. **Generate Embeddings**:\n",
    "   - For each key in the dataset, generates a text embedding.\n",
    "   - For the first 5 values (keys in `image_metadata.json`), loads the corresponding image, preprocesses it, and generates an image embedding if the image file exists.\n",
    "\n",
    "6. **Store Results**:\n",
    "   - Stores the text and image embeddings in a dictionary.\n",
    "   - Saves the dictionary to a new JSON file `embeddings.json`.\n",
    "\n",
    "7. **Print Confirmation**:\n",
    "   - Prints a message confirming that the embeddings have been saved.\n",
    "\n",
    "The script ensures that the embeddings are correctly generated and stored for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0039cd5-b669-41ed-ac13-9bb844e9dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.17.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl (410 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.3/410.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.23.2 safetensors-0.4.3 sentence-transformers-3.0.0 tokenizers-0.19.1 transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torchvision pillow torch git+https://github.com/openai/CLIP.git gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0821f965-3aab-4bf1-a516-2acbc818fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique furniture items: 90\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('image_metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract filenames\n",
    "filenames = [item['filename'] for item in data.values()]\n",
    "\n",
    "# Extract unique furniture items from filenames\n",
    "unique_items = set()\n",
    "for filename in filenames:\n",
    "    item = filename.split('.page')[0]\n",
    "    unique_items.add(item.lower())\n",
    "\n",
    "# Count the number of unique furniture items\n",
    "num_unique_items = len(unique_items)\n",
    "\n",
    "print(f'Total number of unique furniture items: {num_unique_items}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2750fc-b118-45d5-a1f5-995a73d245f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mammut_1', 'lack', 'jules_1', 'smussla', 'klappsta', 'fredrik', 'applaro_2', 'lerhamn', 'leifarne', 'vadholma', 'sigurd', 'preben', 'laiva', 'hemnes', 'vaniljstang', 'nilsove', 'lantliv', 'lantliv_2', 'askholmen', 'nesna', 'vittsjo_1', 'skogsta', 'klingsbo_1', 'perjohan', 'stig', 'gladom', 'vittsjo', 'skogsta_2', 'ekenas', 'vedbo', 'reidar', 'jokkmokk', 'vesken', 'flisat', 'grubban', 'ingolf', 'kyrre', 'poang_2', 'jules_2', 'marius', 'hemnes_2', 'satsumas_2', 'sjalland', 'glenn', 'bjorkudden', 'applaro_3', 'lunnarp', 'nilsolle', 'ivar', 'tobias', 'olivblad', 'pinnig', 'alex', 'ragrund', 'applaro', 'norraryd', 'dalfred', 'tornviken', 'omtanksam', 'ingolf_2', 'vittsjo_2', 'voxlov', 'nordviken', 'norraker', 'nils', 'bernhard', 'falholmen', 'fjallbo', 'kaustby', 'stefan', 'fanbyn', 'teodores', 'trogen', 'bjorkudden_2', 'mammut_2', 'silveran', 'tjusig', 'satsumas', 'herman', 'froset', 'pahl', 'poang_1', 'ronninge', 'bekvam_3017', 'tommaryd', 'sundvik', 'agam', 'yngvar', 'borje', 'lisabo'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cde5eb-0e83-4b43-9940-80dfc5282c9c",
   "metadata": {},
   "source": [
    "## Data Structure: embeddings.json\n",
    "- text_embeddings: prompt embedding\n",
    "- image_embeddings_top5_idx: top 5 relevant images saved as an index of the image_embeddings_array\n",
    "- image_embeddings_all: image embeddings for the entire manual \n",
    "- scores: scores for the top 5 relevant images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ba224c-51da-4842-92de-b6ab51faf677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "# Run this cell if using clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848fb90-aa79-45d6-8544-9cd0758e11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "--------------------GENERATING EMBEDDINGS--------------------\n",
      "--------------------IMAGE EMBEDDINGS USING clip--------------------\n",
      "--------------------TEXT EMBEDDINGS USING word2vec--------------------\n",
      "--------------------DATA TYPE: train--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/Ali/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      " 64%|██████████████████████████████████▎                   | 595/936 [06:16<04:01,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms as transforms\n",
    "from gensim.models import KeyedVectors\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# Load the dataset\n",
    "from ast import literal_eval\n",
    "\n",
    "# image embedding types: [\"resnet-18\", \"clip\"]\n",
    "IMAGE_EMBEDDING_TYPE = \"clip\"\n",
    "# text embedding types: [\"MiniLM-L6\", \"word2vec\"]\n",
    "TEXT_EMBEDDING_TYPE=\"word2vec\"\n",
    "# train test val types: [\"train\", \"test\", \"val\"]\n",
    "TRAIN_TEST_VAL_TYPE = \"train\"\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(f\"--------------------GENERATING EMBEDDINGS--------------------\")\n",
    "print(f\"--------------------IMAGE EMBEDDINGS USING {IMAGE_EMBEDDING_TYPE}--------------------\")\n",
    "print(f\"--------------------TEXT EMBEDDINGS USING {TEXT_EMBEDDING_TYPE}--------------------\")\n",
    "print(f\"--------------------DATA TYPE: {TRAIN_TEST_VAL_TYPE}--------------------\")\n",
    "\n",
    "# Load in data and unwrap it \n",
    "# Function to convert stringified tuple keys back to tuples\n",
    "def unwrap_keys(mapping):\n",
    "    return {literal_eval(k): v for k, v in mapping.items()}\n",
    "\n",
    "# Load the JSON file\n",
    "with open(f'augmented_data/augmented_dataset_{TRAIN_TEST_VAL_TYPE}.json', 'r') as json_file:\n",
    "    data_from_json = json.load(json_file)\n",
    "\n",
    "# print(data_from_json)\n",
    "# Unwrap the keys to their original tuple format\n",
    "dataset = unwrap_keys(data_from_json)\n",
    "\n",
    "# Load image metadata\n",
    "with open('image_metadata.json', 'r') as file:\n",
    "    image_metadata = json.load(file)\n",
    "\n",
    "# Initialize text embedding model\n",
    "if TEXT_EMBEDDING_TYPE == \"MiniLM-L6\":\n",
    "    text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "elif TEXT_EMBEDDING_TYPE == \"word2vec\":\n",
    "    text_model = KeyedVectors.load_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "else:\n",
    "    raise Exception(\"Invalid TEXT_EMBEDDING_TYPE. Supported types are MiniLM-L6.\")\n",
    "\n",
    "# Initialize image embedding model\n",
    "image_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "image_model.eval()\n",
    "\n",
    "# Preprocessing transformations for the images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to generate word2vec embeddings \n",
    "def get_word2vec_embedding(text, model):\n",
    "    words = text.split()\n",
    "    valid_word_vectors = [model[word] for word in words if word in model]\n",
    "\n",
    "    if not valid_word_vectors:\n",
    "        # Handle the case where none of the words are in the model\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Compute the average of the word vectors\n",
    "    return np.mean(valid_word_vectors, axis=0).tolist()\n",
    "\n",
    "# Function to generate image embeddings\n",
    "def generate_image_embedding(image_path):\n",
    "    if IMAGE_EMBEDDING_TYPE == \"resnet-18\":\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = image_model(image_tensor).numpy().flatten()\n",
    "        return image_embedding.tolist()\n",
    "    elif IMAGE_EMBEDDING_TYPE == \"clip\":\n",
    "        image = clip_preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            # Normalize the embeddings\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            # Convert the tensor to a numpy array if needed\n",
    "            image_embeddings = image_features.cpu().numpy()\n",
    "            return image_embeddings.tolist()\n",
    "    else:\n",
    "        raise Exception(\"Invalid IMAGE_EMBEDDING_TYPE. Supported types are resnet-18, CLIP.\") \n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = {}\n",
    "image_embeddings = {}\n",
    "for key, values in tqdm(dataset.items()):\n",
    "    # Generate text embedding for the key\n",
    "    if TEXT_EMBEDDING_TYPE == \"MiniLM-L6\":\n",
    "        text_embedding = text_model.encode(key[0]).tolist()\n",
    "    elif TEXT_EMBEDDING_TYPE == \"word2vec\":\n",
    "        text_embedding = get_word2vec_embedding(key[0], text_model)\n",
    "        # print(type(text_embedding))\n",
    "    else:\n",
    "        raise Exception(\"Invalid text embedding model. Supported types are word2vec, MiniLM-L6\")\n",
    "    # Generate image embeddings for the first 5 values\n",
    "    # image_embeddings = []\n",
    "    word_list = key[0].split()\n",
    "    for word in word_list:\n",
    "        for item in unique_items:\n",
    "            if word.lower() == item:\n",
    "                furniture_name = word.lower()\n",
    "    # print(furniture_name)\n",
    "    # furniture_name = image_metadata.get(str(values[0]))['filename'].split('.page')[0]\n",
    "    # for i in range(5):\n",
    "    #     if i < len(values):\n",
    "    #         image_info = image_metadata.get(str(values[i]))\n",
    "    #         if image_info:\n",
    "    #             image_path = image_info['filename']\n",
    "                \n",
    "    #             full_image_path = os.path.join('data_wiki', image_path)\n",
    "    #             if os.path.exists(full_image_path):\n",
    "    #                 image_embedding = generate_image_embedding(full_image_path)\n",
    "    #                 image_embeddings.append(image_embedding)\n",
    "    #             else:\n",
    "    #                 print(\"Can't find image: \", full_image_path)\n",
    "    #         else:\n",
    "    #             print(\"Error for prompt: \", str(key))\n",
    "    \n",
    "    # Add additional embeddings for all uiuds containing 'name'\n",
    "    all_image_embeddings = []\n",
    "    is_first_img = 0\n",
    "    list_of_idxs = values[\"idxs_and_scores\"]\n",
    "    for uiud, image_info in image_metadata.items():\n",
    "        # print(f\"{furniture_name} - {image_info['filename'].split('.page')[0]}\") \n",
    "        if furniture_name == image_info['filename'].split('.page')[0]:\n",
    "            # print(furniture_name) \n",
    "            if is_first_img == 0:\n",
    "                embed_idxs = [x - int(uiud) for x in list_of_idxs[:5]]\n",
    "                # print(f\"{furniture_name} + {int(uiud)}\")\n",
    "                is_first_img += 1 \n",
    "            image_path = image_info['filename']\n",
    "            full_image_path = os.path.join('data_wiki', image_path)\n",
    "            # print(full_image_path)\n",
    "            if os.path.exists(full_image_path):\n",
    "                image_embedding = generate_image_embedding(full_image_path)\n",
    "                all_image_embeddings.append(image_embedding)\n",
    "            else:\n",
    "                print(\"Can't find image: \", full_image_path)\n",
    "\n",
    "    # Sanity Check - Indices Should not be out of Bounds \n",
    "    for x in embed_idxs:\n",
    "        if x >= len(all_image_embeddings) or x < 0:\n",
    "            print(f\"Error - index out of bounds for sample {furniture_name}\")\n",
    "            \n",
    "    # Save Embeddings\n",
    "    image_scores = list_of_idxs[5:]\n",
    "    embeddings[key] = {\n",
    "        'text_embedding': text_embedding,\n",
    "        'image_embeddings_top5_idx': embed_idxs,\n",
    "        'image_embeddings_key': furniture_name,\n",
    "        'scores': image_scores,\n",
    "    }\n",
    "    if furniture_name not in image_embeddings.keys():\n",
    "        image_embeddings[furniture_name] = all_image_embeddings\n",
    "        \n",
    "    \n",
    "\n",
    "# Save the embeddings to a JSON file\n",
    "# print(list(embeddings.keys())[0])\n",
    "# print(list(embeddings.values())[0])\n",
    "print(\"Size of image embedding\", len(list(image_embeddings['alex'][0])))\n",
    "print(\"Size of text embedding\", len(list(embeddings.values())[0]['text_embedding']))\n",
    "# print(list(embeddings.values())[0]['image_embeddings_top5_idx']\n",
    "\n",
    "def remap_keys(mapping):\n",
    "    return {str(k): v for k, v in mapping.items()}\n",
    "    \n",
    "with open(f'embeddings/{TEXT_EMBEDDING_TYPE.lower()}_{IMAGE_EMBEDDING_TYPE.lower()}_embeddings_{TRAIN_TEST_VAL_TYPE}_aug.json', 'w') as file:\n",
    "    json.dump(remap_keys(embeddings), file, indent=4)\n",
    "\n",
    "with open(f'embeddings/{TEXT_EMBEDDING_TYPE.lower()}_{IMAGE_EMBEDDING_TYPE.lower()}_embeddings_raw_{TRAIN_TEST_VAL_TYPE}_aug.json','w') as file:\n",
    "    json.dump(image_embeddings, file, indent=4)\n",
    "\n",
    "print(f\"Embeddings have been saved to 'embeddings/{TEXT_EMBEDDING_TYPE.lower()}_{IMAGE_EMBEDDING_TYPE.lower()}_embeddings_{TRAIN_TEST_VAL_TYPE}_aug.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8da29c5b-caa0-4cf3-9000-9857f9b010e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1276\n"
     ]
    }
   ],
   "source": [
    "total = 936+170+170\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c161da4e-fec2-48a2-9fc8-86d508bdb3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of image embedding 1000\n",
      "Size of text embedding (300,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of image embedding\", len(list(image_embeddings['alex'][0])))\n",
    "print(\"Size of text embedding\", len(list(embeddings.values())[0]['text_embedding']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cefd4c-2a84-4b4d-afb1-d5cbd1c57303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
