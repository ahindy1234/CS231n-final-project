{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc6bc4-bd91-4e03-96f8-4342e566f295",
   "metadata": {},
   "source": [
    "## Detailed Report on Data Augmentation Methods\n",
    "\n",
    "- Controlled Synonym Replacement: This method replaces words in the sentence with their synonyms, ensuring that the replacement words are valid and meaningful. This increases variability while retaining the semantic meaning.\n",
    "\n",
    "- Back-Translation Paraphrasing: This method uses back-translation to generate paraphrases. The sentence is translated to another language and then back to English. By using multiple languages (French, Spanish, German), we generate diverse paraphrases while retaining the original meaning.\n",
    "\n",
    "- Template-Based Augmentation: This method uses a set of predefined templates to create variations of the original sentence. By inserting the original sentence into different templates, we generate meaningful variations that enhance the dataset.\n",
    "\n",
    "- Multiple Augmentation Rounds: The dataset is augmented multiple times to generate a larger dataset. Each round includes synonym replacement, back-translation, and template-based augmentation to ensure comprehensive augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e342587-dd31-4357-b678-8a6e1d8621a6",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4a64d8-de6c-4c2a-ac72-e6470ff9e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob nltk sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21738e74-9ed3-4b58-91dd-078bf23b296a",
   "metadata": {},
   "source": [
    "## Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dcc5c4f-0ad4-4b9e-b7ef-088d2a7b0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format: prompt (key) -> top-5 most relevant uiuds (value) [1 (most relevant) to 5 (5th most relevant)]\n",
    "dataset = {\n",
    "    \"what is the Alex desk\": [923, 924, 925, 926, 927],\n",
    "    \"for the Alex desk, what are the warnings I should know of?\": [924, 923, 925, 926, 927],\n",
    "    \"for the Alex desk, what parts do I need?\": [925, 924, 923, 926, 927],\n",
    "    \"for the Alex desk, what is the first step?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, what is the second step?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, how many nails do I need for step one?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, how many parts do I need for step two?\": [926, 925, 924, 923, 927], \n",
    "    \"what is the Pahl desk\": [915, 916, 917, 918, 919],\n",
    "    \"for the Pahl desk, what are the warnings I should know of?\": [916, 915, 917, 918, 919],\n",
    "    \"for the Pahl desk, what parts do I need?\": [916, 915, 916, 918, 919],\n",
    "    \"for the Pahl desk, what is the first step?\": [917, 915, 916, 918, 919],\n",
    "    \"for the Pahl desk, what is the second step?\": [918, 915, 916, 917, 919],\n",
    "    \"for the Pahl desk, how many nails do I need for step one?\": [917, 915, 916, 918, 919],\n",
    "    \"for the Pahl desk, how many parts do I need for step two?\": [918, 915, 916, 917, 919], \n",
    "    \"what is the Fredrik desk\": [907, 908, 909, 910, 911],\n",
    "    \"for the Fredrik desk, what are the warnings I should know of?\": [908, 907, 909, 910, 911],\n",
    "    \"for the Fredrik desk, what parts do I need?\": [908, 907, 909, 910, 911],\n",
    "    \"for the Fredrik desk, what is the first step?\": [909, 907, 908, 909, 910],\n",
    "    \"for the Fredrik desk, what is the second step?\": [909, 907, 908, 909, 910],\n",
    "    \"for the Fredrik desk, what tool do I need for step one?\": [909, 907, 908, 909, 910],\n",
    "    \"for the Fredrik desk, how many parts do I need for step two?\": [909, 907, 908, 909, 910],\n",
    "    \"what is the Flisat desk\": [887, 888, 889, 890, 891],\n",
    "    \"for the Flisat desk, what are the warnings I should know of?\": [888, 887, 889, 890, 891],\n",
    "    \"for the Flisat desk, what parts do I need?\": [889, 887, 888, 890, 891],\n",
    "    \"for the Flisat desk, what is the first step?\": [890, 887, 889, 888, 891],\n",
    "    \"for the Flisat desk, what is the second step?\": [891, 887, 889, 888, 890],\n",
    "    \"for the Flisat desk, how many nails do I need for step one?\": [890, 887, 889, 888, 891],\n",
    "    \"for the Flisat desk, how many parts do I need for step two?\": [891, 887, 889, 888, 890],  \n",
    "    \"what is the vittsjo shelf\": [871, 876, 885, 884, 883],\n",
    "    \"for the vittsjo shelf, what are the warnings I should know of?\": [872, 873, 874, 875, 871],\n",
    "    \"for the vittsjo shelf, what parts do I need?\": [876, 871, 885, 884, 883],\n",
    "    \"for the vittsjo shelf, what is the first step?\": [877, 876, 871, 885, 878],\n",
    "    \"for the vittsjo shelf, what is the second step?\": [878, 871, 885, 884, 883],\n",
    "    \"for the vittsjo shelf, what pieces do I need for step one?\": [877, 876, 871, 885, 878],\n",
    "    \"for the vittsjo shelf, how many parts do I need for step two?\": [878, 871, 885, 884, 883], \n",
    "    \"what is the vaniljstang shelf\": [859, 860, 861, 869, 870],\n",
    "    \"for the vaniljstang shelf, what are the warnings I should know of?\": [860, 859, 861, 869, 870],\n",
    "    \"for the vaniljstang shelf, what parts do I need?\": [861, 859, 862, 869, 870],\n",
    "    \"for the vaniljstang shelf, what is the first step?\": [862, 859, 861, 863, 870],\n",
    "    \"for the vaniljstang shelf, what is the second step?\": [863, 859, 862, 861, 870],\n",
    "    \"for the vaniljstang shelf, what pieces do I need for step one?\": [862, 859, 861, 863, 870],\n",
    "    \"for the vaniljstang shelf, how many parts do I need for step two?\": [863, 859, 862, 861, 870],\n",
    "    \"what is the satsumas furniture\": [847, 848, 849, 857, 858],\n",
    "    \"for the satsumas furniture, what are the warnings I should know of?\": [848, 847, 849, 857, 858],\n",
    "    \"for the satsumas furniture, what parts do I need?\": [849, 848, 847, 857, 858],\n",
    "    \"for the satsumas furniture, what is the first step?\": [850, 847, 851, 857, 858],\n",
    "    \"for the satsumas furniture, what is the second step?\": [850, 847, 851, 857, 858],\n",
    "    \"for the satsumas furniture, what pieces do I need for step one?\": [850, 847, 851, 857, 858],\n",
    "    \"for the satsumas furniture, how many parts do I need for step two?\": [850, 847, 851, 857, 858],  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ccd03-c3a6-4195-a374-c8d1a21255a7",
   "metadata": {},
   "source": [
    "## Naive Augmentation - make the dataset 10x larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48691f21-dbe4-43b3-8ec6-3f76c91056f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 4242.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the augmented dataset: 588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "# Example dataset\n",
    "# dataset = {\n",
    "#     \"what is the Alex desk\": [923, 924, 925, 926, 927],\n",
    "#     \"for the Alex desk, what are the warnings I should know of?\": [924, 923, 925, 926, 927],\n",
    "#     \"for the Alex desk, what parts do I need?\": [925, 924, 923, 926, 927],\n",
    "#     \"for the Alex desk, what is the first step?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, what is the second step?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, how many nails do I need for step one?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, how many parts do I need for step two?\": [926, 925, 924, 923, 927],\n",
    "# }\n",
    "\n",
    "# Synonym replacement function ensuring semantic meaning\n",
    "def synonym_replacement(sentence):\n",
    "    words = sentence.split()\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                new_sentence.append(synonym)\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "# Paraphrasing using back-translation with multiple languages\n",
    "def back_translate(sentence, languages=['fr', 'es', 'de']):\n",
    "    translations = []\n",
    "    try:\n",
    "        blob = TextBlob(sentence)\n",
    "        for lang in languages:\n",
    "            translated = str(blob.translate(to=lang).translate(to='en'))\n",
    "            if translated != sentence:\n",
    "                translations.append(translated)\n",
    "    except Exception as e:\n",
    "        # If there's an error in translation, return the original sentence\n",
    "        translations.append(sentence)\n",
    "    return translations\n",
    "\n",
    "# Template-based augmentation\n",
    "def template_augmentation(sentence):\n",
    "    templates = [\n",
    "        \"Can you tell me about {}?\",\n",
    "        \"I would like to know about {}.\",\n",
    "        \"What can you say about {}?\",\n",
    "        \"Provide details about {}.\",\n",
    "        \"{} - could you elaborate?\",\n",
    "        \"Please explain {} in detail.\",\n",
    "        \"What information is available on {}?\",\n",
    "        \"Could you tell me about {}?\",\n",
    "        \"I need information on {}.\",\n",
    "        \"Could you provide more details on {}?\"\n",
    "    ]\n",
    "    augmented_sentences = []\n",
    "    for template in templates:\n",
    "        augmented_sentences.append(template.format(sentence))\n",
    "    return augmented_sentences\n",
    "\n",
    "# Function to augment the dataset multiple times\n",
    "def augment_dataset(dataset, rounds=1):\n",
    "    augmented_dataset = dataset.copy()\n",
    "    for _ in range(rounds):\n",
    "        new_entries = {}\n",
    "        for question, ids in tqdm(augmented_dataset.items()):\n",
    "            # Synonym Replacement\n",
    "            augmented_sentence = synonym_replacement(question)\n",
    "            if augmented_sentence != question:\n",
    "                new_entries[augmented_sentence] = ids\n",
    "            \n",
    "            # Back-Translation Paraphrasing\n",
    "            augmented_sentences = back_translate(question)\n",
    "            for augmented_sentence in augmented_sentences:\n",
    "                if augmented_sentence != question:\n",
    "                    new_entries[augmented_sentence] = ids\n",
    "            \n",
    "            # Template-Based Augmentation\n",
    "            augmented_sentences = template_augmentation(question)\n",
    "            for augmented_sentence in augmented_sentences:\n",
    "                if augmented_sentence != question:\n",
    "                    new_entries[augmented_sentence] = ids\n",
    "        \n",
    "        augmented_dataset.update(new_entries)\n",
    "    return augmented_dataset\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_dataset = augment_dataset(dataset, rounds=1)\n",
    "\n",
    "# Combine the original dataset with the augmented dataset\n",
    "final_dataset = {**dataset, **augmented_dataset}\n",
    "\n",
    "# Save the augmented dataset to a JSON file including the original dataset\n",
    "with open('augmented_dataset.json', 'w') as f:\n",
    "    json.dump(final_dataset, f, indent=4)\n",
    "\n",
    "# Load the augmented dataset from the JSON file\n",
    "with open('augmented_dataset.json', 'r') as f:\n",
    "    loaded_augmented_dataset = json.load(f)\n",
    "\n",
    "# Print the loaded augmented dataset\n",
    "# print(json.dumps(loaded_augmented_dataset, indent=4))\n",
    "\n",
    "# Print the length of the augmented dataset\n",
    "print(f\"Length of the augmented dataset: {len(loaded_augmented_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21855a6b-d9c8-4790-b9a1-840d8b65a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the augmented dataset: 294\n"
     ]
    }
   ],
   "source": [
    "# Load the augmented dataset from the JSON file\n",
    "with open('augmented_dataset.json', 'r') as f:\n",
    "    loaded_augmented_dataset = json.load(f)\n",
    "\n",
    "# Print the loaded augmented dataset\n",
    "# print(json.dumps(loaded_augmented_dataset, indent=4))\n",
    "\n",
    "# Print the length of the augmented dataset\n",
    "print(f\"Length of the augmented dataset: {len(loaded_augmented_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdd9b9-afed-47d2-a71e-cf61a7602f08",
   "metadata": {},
   "source": [
    "## Augment to 500 samples with train, test, split\n",
    "\n",
    "- Saved as `augmented_train_dataset.json`, `augmented_test_dataset.json`, `augmented_val_dataset.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fc14e80-1bdf-4b24-ade0-e4abf381901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of augmented train dataset: 348\n",
      "Length of augmented val dataset: 84\n",
      "Length of augmented test dataset: 136\n",
      "\n",
      "Random samples from augmented train dataset:\n",
      "[('Provide more insights about for the vittsjo shelf, what parts do I need?.', [876, 871, 885, 884, 883]), ('Can you elaborate more on for the vittsjo shelf, what parts do I need??', [876, 871, 885, 884, 883]), ('Give me an overview of for the Alex desk, how many nails do I need for step one?.', [926, 925, 924, 923, 927]), ('for the Alex desk, how many parts do I need for step two?', [926, 925, 924, 923, 927]), ('for the Pahl desk, how many nails do I need for step one?', [917, 915, 916, 918, 919]), ('for the Pahl desk, how many parts do I need for step two?', [918, 915, 916, 917, 919]), ('Provide details about for the Flisat desk, what parts do I need?.', [889, 887, 888, 890, 891]), ('What do you know about for the vaniljstang shelf, what is the first step??', [862, 859, 861, 863, 870]), ('for the Fredrik desk, what parts do I need? - could you elaborate?', [908, 907, 909, 910, 911]), ('Break down for the vittsjo shelf, what is the second step? for me.', [878, 871, 885, 884, 883])]\n",
      "\n",
      "Random samples from augmented val dataset:\n",
      "[('what is the Flisat desk', [887, 888, 889, 890, 891]), ('for the vaniljstang shelf, what is the first step?', [862, 859, 861, 863, 870]), ('for the vaniljstang shelf, what pieces do I need for step one?', [862, 859, 861, 863, 870]), ('Provide more insights about for the vittsjo shelf, what pieces do I need for step one?.', [877, 876, 871, 885, 878]), ('for the vittsjo shelf, what is the first step?', [877, 876, 871, 885, 878]), ('What can you say about for the vittsjo shelf, what pieces do I need for step one??', [877, 876, 871, 885, 878]), ('I would like to know about for the Fredrik desk, how many parts do I need for step two?.', [909, 907, 908, 909, 910]), ('for the vaniljstang shelf, what is the second step?', [863, 859, 862, 861, 870]), ('for the satsumas furniture, what pieces do I need for step one?', [850, 847, 851, 857, 858]), ('Explain what is the Fredrik desk to me.', [907, 908, 909, 910, 911])]\n",
      "\n",
      "Random samples from augmented test dataset:\n",
      "[('Elaborate on for the Flisat desk, how many parts do I need for step two? please.', [891, 887, 889, 888, 890]), ('Break down for the vittsjo shelf, what is the first step? for me.', [877, 876, 871, 885, 878]), ('Give me an overview of for the vittsjo shelf, what is the first step?.', [877, 876, 871, 885, 878]), ('for the Flisat desk, how many parts do I need for step two?', [891, 887, 889, 888, 890]), ('Can you elaborate more on for the vittsjo shelf, what is the first step??', [877, 876, 871, 885, 878]), ('what is the Flisat desk', [887, 888, 889, 890, 891]), ('Please explain for the Flisat desk, how many parts do I need for step two? in detail.', [891, 887, 889, 888, 890]), ('Tell me something regarding for the Fredrik desk, what is the first step?.', [909, 907, 908, 909, 910]), ('What can you say about for the Flisat desk, what is the second step??', [891, 887, 889, 888, 890]), ('Could you shed light on for the Fredrik desk, what tool do I need for step one??', [909, 907, 908, 909, 910])]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "# dataset = {\n",
    "#     \"what is the Alex desk\": [923, 924, 925, 926, 927],\n",
    "#     \"for the Alex desk, what are the warnings I should know of?\": [924, 923, 925, 926, 927],\n",
    "#     \"for the Alex desk, what parts do I need?\": [925, 924, 923, 926, 927],\n",
    "#     \"for the Alex desk, what is the first step?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, what is the second step?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, how many nails do I need for step one?\": [926, 925, 924, 923, 927],\n",
    "#     \"for the Alex desk, how many parts do I need for step two?\": [926, 925, 924, 923, 927],\n",
    "# }\n",
    "\n",
    "# Synonym replacement function ensuring semantic meaning\n",
    "def synonym_replacement(sentence):\n",
    "    words = sentence.split()\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                new_sentence.append(synonym)\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "# Paraphrasing using back-translation with multiple languages\n",
    "def back_translate(sentence, languages=['fr', 'es', 'de']):\n",
    "    translations = []\n",
    "    try:\n",
    "        blob = TextBlob(sentence)\n",
    "        for lang in languages:\n",
    "            translated = str(blob.translate(to=lang).translate(to='en'))\n",
    "            if translated != sentence:\n",
    "                translations.append(translated)\n",
    "    except Exception as e:\n",
    "        # If there's an error in translation, return the original sentence\n",
    "        translations.append(sentence)\n",
    "    return translations\n",
    "\n",
    "# Expanded Template array\n",
    "templates = [\n",
    "    \"Can you tell me about {}?\",\n",
    "    \"I would like to know about {}.\",\n",
    "    \"What can you say about {}?\",\n",
    "    \"Provide details about {}.\",\n",
    "    \"{} - could you elaborate?\",\n",
    "    \"Please explain {} in detail.\",\n",
    "    \"What information is available on {}?\",\n",
    "    \"Could you tell me about {}?\",\n",
    "    \"I need information on {}.\",\n",
    "    \"Could you provide more details on {}?\",\n",
    "    \"Tell me something regarding {}.\",\n",
    "    \"Give me an explanation about {}.\",\n",
    "    \"Can you elaborate more on {}?\",\n",
    "    \"Provide more insights about {}.\",\n",
    "    \"{} - can you give further details?\",\n",
    "    \"Could you explain {} more thoroughly?\",\n",
    "    \"What do you know about {}?\",\n",
    "    \"I want to understand {} better.\",\n",
    "    \"{} - could you clarify?\",\n",
    "    \"Please provide details regarding {}.\",\n",
    "    \"Explain {} to me.\",\n",
    "    \"Elaborate on {} please.\",\n",
    "    \"Give me an overview of {}.\",\n",
    "    \"Tell me what you know about {}.\",\n",
    "    \"Could you shed light on {}?\",\n",
    "    \"Clarify {} for me.\",\n",
    "    \"Discuss {} in depth.\",\n",
    "    \"{} - what's the scoop?\",\n",
    "    \"Break down {} for me.\",\n",
    "]\n",
    "\n",
    "# Function to randomly select a template\n",
    "def random_template(templates):\n",
    "    return random.choice(templates)\n",
    "\n",
    "# Function to apply augmentation based on dataset type\n",
    "def apply_augmentation(dataset, augment_func, rounds=10):\n",
    "    augmented_dataset = {}\n",
    "    for _ in range(rounds):\n",
    "        for question, ids in dataset:\n",
    "            augmented_sentences = augment_func(question)\n",
    "            for augmented_sentence in augmented_sentences:\n",
    "                if augmented_sentence not in augmented_dataset:\n",
    "                    augmented_dataset[augmented_sentence] = ids\n",
    "    return augmented_dataset\n",
    "\n",
    "# Different augmentation strategies for train, val, test sets\n",
    "def augment_train_set(question):\n",
    "    selected_template = random_template(templates)\n",
    "    augmented_sentence = selected_template.format(question)\n",
    "    return [augmented_sentence]\n",
    "\n",
    "def augment_val_set(question):\n",
    "    selected_template = random_template(templates)\n",
    "    augmented_sentence = selected_template.format(question)\n",
    "    return [augmented_sentence]\n",
    "\n",
    "def augment_test_set(question):\n",
    "    selected_template = random_template(templates)\n",
    "    augmented_sentence = selected_template.format(question)\n",
    "    return [augmented_sentence]\n",
    "\n",
    "# Split dataset into train, val, test sets\n",
    "train_dataset, test_dataset = train_test_split(list(dataset.items()), test_size=0.2, random_state=42)\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Apply augmentation to each dataset\n",
    "augmented_train = apply_augmentation(train_dataset, augment_train_set, rounds=10)\n",
    "augmented_val = apply_augmentation(val_dataset, augment_val_set, rounds=10)\n",
    "augmented_test = apply_augmentation(test_dataset, augment_test_set, rounds=10)\n",
    "\n",
    "# Combine augmented datasets into final train, val, test sets\n",
    "final_train_dataset = {**dataset, **augmented_train}\n",
    "final_val_dataset = {**dataset, **augmented_val}\n",
    "final_test_dataset = {**dataset, **augmented_test}\n",
    "\n",
    "# Save the augmented datasets to JSON files\n",
    "with open('augmented_train_dataset.json', 'w') as f:\n",
    "    json.dump(final_train_dataset, f, indent=4)\n",
    "\n",
    "with open('augmented_val_dataset.json', 'w') as f:\n",
    "    json.dump(final_val_dataset, f, indent=4)\n",
    "\n",
    "with open('augmented_test_dataset.json', 'w') as f:\n",
    "    json.dump(final_test_dataset, f, indent=4)\n",
    "\n",
    "# Print lengths of augmented datasets\n",
    "print(f\"Length of augmented train dataset: {len(final_train_dataset)}\")\n",
    "print(f\"Length of augmented val dataset: {len(final_val_dataset)}\")\n",
    "print(f\"Length of augmented test dataset: {len(final_test_dataset)}\")\n",
    "\n",
    "# Print 10 random samples from each dataset for verification\n",
    "print(\"\\nRandom samples from augmented train dataset:\")\n",
    "print(random.sample(list(final_train_dataset.items()), 10))\n",
    "print(\"\\nRandom samples from augmented val dataset:\")\n",
    "print(random.sample(list(final_val_dataset.items()), 10))\n",
    "print(\"\\nRandom samples from augmented test dataset:\")\n",
    "print(random.sample(list(final_test_dataset.items()), 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e056e6-79bd-4fc8-aeca-6b35003c3dac",
   "metadata": {},
   "source": [
    "## Augment to above 1 million samples given 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2715216e-5fde-4b08-9cf9-407d1f9d5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the augmented dataset: 1123587\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example dataset\n",
    "dataset = {\n",
    "    \"what is the Alex desk\": [923, 924, 925, 926, 927],\n",
    "    \"for the Alex desk, what are the warnings I should know of?\": [924, 923, 925, 926, 927],\n",
    "    \"for the Alex desk, what parts do I need?\": [925, 924, 923, 926, 927],\n",
    "    \"for the Alex desk, what is the first step?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, what is the second step?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, how many nails do I need for step one?\": [926, 925, 924, 923, 927],\n",
    "    \"for the Alex desk, how many parts do I need for step two?\": [926, 925, 924, 923, 927],\n",
    "}\n",
    "\n",
    "# Synonym replacement function ensuring semantic meaning\n",
    "def synonym_replacement(sentence):\n",
    "    words = sentence.split()\n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != word and synonym.isalpha():\n",
    "                new_sentence.append(synonym)\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "# Paraphrasing using back-translation with multiple languages\n",
    "def back_translate(sentence, languages=['fr', 'es', 'de']):\n",
    "    translations = []\n",
    "    try:\n",
    "        blob = TextBlob(sentence)\n",
    "        for lang in languages:\n",
    "            translated = str(blob.translate(to=lang).translate(to='en'))\n",
    "            if translated != sentence:\n",
    "                translations.append(translated)\n",
    "    except Exception as e:\n",
    "        # If there's an error in translation, return the original sentence\n",
    "        translations.append(sentence)\n",
    "    return translations\n",
    "\n",
    "# Template-based augmentation\n",
    "def template_augmentation(sentence):\n",
    "    templates = [\n",
    "        \"Can you tell me about {}?\",\n",
    "        \"I would like to know about {}.\",\n",
    "        \"What can you say about {}?\",\n",
    "        \"Provide details about {}.\",\n",
    "        \"{} - could you elaborate?\",\n",
    "        \"Please explain {} in detail.\",\n",
    "        \"What information is available on {}?\",\n",
    "        \"Could you tell me about {}?\",\n",
    "        \"I need information on {}.\",\n",
    "        \"Could you provide more details on {}?\"\n",
    "    ]\n",
    "    augmented_sentences = []\n",
    "    for template in templates:\n",
    "        augmented_sentences.append(template.format(sentence))\n",
    "    return augmented_sentences\n",
    "\n",
    "# Function to augment the dataset multiple times\n",
    "def augment_dataset(dataset, rounds=1):\n",
    "    augmented_dataset = dataset.copy()\n",
    "    for _ in range(rounds):\n",
    "        new_entries = {}\n",
    "        for question, ids in augmented_dataset.items():\n",
    "            # Synonym Replacement\n",
    "            augmented_sentence = synonym_replacement(question)\n",
    "            if augmented_sentence != question:\n",
    "                new_entries[augmented_sentence] = ids\n",
    "            \n",
    "            # Back-Translation Paraphrasing\n",
    "            augmented_sentences = back_translate(question)\n",
    "            for augmented_sentence in augmented_sentences:\n",
    "                if augmented_sentence != question:\n",
    "                    new_entries[augmented_sentence] = ids\n",
    "            \n",
    "            # Template-Based Augmentation\n",
    "            augmented_sentences = template_augmentation(question)\n",
    "            for augmented_sentence in augmented_sentences:\n",
    "                if augmented_sentence != question:\n",
    "                    new_entries[augmented_sentence] = ids\n",
    "        \n",
    "        augmented_dataset.update(new_entries)\n",
    "    return augmented_dataset\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_dataset = augment_dataset(dataset, rounds=5)\n",
    "\n",
    "# Save the augmented dataset to a JSON file including the original dataset\n",
    "final_dataset = {**dataset, **augmented_dataset}\n",
    "with open('augmented_dataset.json', 'w') as f:\n",
    "    json.dump(final_dataset, f, indent=4)\n",
    "\n",
    "# Print the augmented dataset\n",
    "# print(json.dumps(final_dataset, indent=4))\n",
    "\n",
    "# Print the length of the augmented dataset\n",
    "print(f\"Length of the augmented dataset: {len(final_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73722567-333d-4894-8720-b2df39d959f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
