{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "DIRECTORY = \"/content/drive/My Drive/cs231n/final project\"\n",
        "%cd $DIRECTORY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWpbp0N9fkPX",
        "outputId": "1a6f5f95-aa31-4e76-b834-d11abcd39a18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/cs231n/final project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFy0pdwMfZ-9",
        "outputId": "47515c89-6961-46f0-bc74-3b3ee05d5b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss.\n",
        "print_every = 100\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imageEmbeddingSize, queryTextEmbeddingSize = 1000, 384"
      ],
      "metadata": {
        "id": "wVssOaGbDPUI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "# Load in data and unwrap it\n",
        "# Function to convert stringified tuple keys back to tuples\n",
        "def unwrap_keys(mapping):\n",
        "    return {literal_eval(k): v for k, v in mapping.items()}\n",
        "\n",
        "# Load the JSON file\n",
        "with open('./embeddings.json', 'r') as json_file:\n",
        "    data_from_json = json.load(json_file)\n",
        "\n",
        "# print(data_from_json)\n",
        "# Unwrap the keys to their original tuple format\n",
        "unwrapped_data = unwrap_keys(data_from_json)"
      ],
      "metadata": {
        "id": "BWelGQatLZ7y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_key = \"text_embedding\"\n",
        "image_key = \"image_embeddings_all\"\n",
        "score_key = \"scores\"\n",
        "image_embeddings_top_5 = \"image_embeddings_top5_idx\"\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "y_output = []\n",
        "X_image_eval = []\n",
        "X_text_embed_eval = []\n",
        "y_eval = []\n",
        "prompts = []\n",
        "for key, sub_dataset in unwrapped_data.items():\n",
        "  # pprint.pp(sub_dataset)\n",
        "  text_embedding = sub_dataset[text_key]\n",
        "  image_embedding = sub_dataset[image_key]\n",
        "  # print(len(image_embedding))\n",
        "  scores = sub_dataset[score_key]\n",
        "  top5 = sub_dataset[image_embeddings_top_5]\n",
        "  # print(len(image_embedding), len(text_embedding), len(scores))\n",
        "  # if len(image_embedding) != len(scores):\n",
        "  #   continue\n",
        "  # print(top5, len(image_embedding))\n",
        "  # print(key)\n",
        "  for i in range(1):\n",
        "    idx = top5[i]\n",
        "    # print(top5, len(image_embedding))\n",
        "    if idx >= len(image_embedding):\n",
        "      # print(top5, len(image_embedding))\n",
        "      print(key)\n",
        "      continue\n",
        "    image_embeddings.append(image_embedding[idx])\n",
        "    text_embeddings.append(text_embedding)\n",
        "    y_output.append(scores[i])\n",
        "  if top5[0] == 0:\n",
        "    continue\n",
        "  X_image_eval.append(image_embedding)\n",
        "  X_text_embed_eval.append([text_embedding] * len(image_embedding))\n",
        "  y_eval.append(top5)\n",
        "  prompts.append(key)\n",
        "\n",
        "# print(y_output)\n",
        "N = len(image_embeddings)\n",
        "print(len(image_embeddings), len(text_embeddings), len(y_output))\n",
        "# print(y_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_A5Lsxeq6-w",
        "outputId": "25cccd8e-c9e5-462d-eb29-826e6cb77fc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "224 224 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_images = torch.tensor(image_embeddings)\n",
        "X_queries = torch.tensor(text_embeddings)\n",
        "y = torch.tensor(y_output)\n",
        "print(X_images.shape, X_queries.shape, y.shape)\n",
        "dataset = TensorDataset(X_images, X_queries, y)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# print(X_images.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJI9LC23qRFj",
        "outputId": "4777a836-1832-4214-cb75-7c5fec75e307"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([224, 1000]) torch.Size([224, 384]) torch.Size([224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingProjectionNN(nn.Module):\n",
        "    def __init__(self, embeddingSize):\n",
        "      super().__init__()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(embeddingSize, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(512, 512),\n",
        "          nn.LayerNorm(512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # input (N, E)\n",
        "      projection = self.linear_relu_stack(x)\n",
        "      return projection\n",
        "\n",
        "class RetrieverNN(nn.Module):\n",
        "  def __init__(self, imageEmbedding, queryEmbedding):\n",
        "    super().__init__()\n",
        "    self.imageProj = EmbeddingProjectionNN(imageEmbedding)\n",
        "    self.queryProj = EmbeddingProjectionNN(queryEmbedding)\n",
        "    self.similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, images, query):\n",
        "    imageProj = self.imageProj(images)\n",
        "    queryProj = self.queryProj(query)\n",
        "    similarities = self.similarity(imageProj, queryProj)\n",
        "    # make similarities a prob scores between 0 and 1\n",
        "    scores = similarities * 0.5 + 0.5\n",
        "    return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "kvJJm9zsf2BO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.BCELoss()\n",
        "def train_part34(model, optimizer, epochs=10, scheduler = None):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for imageEmbeddings, queryEmbedding, y in dataloader:\n",
        "          # (B, E1), (B, E2), (B,)\n",
        "          model.train()  # put model to training mode\n",
        "          imageEmbeddings = imageEmbeddings.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "          queryEmbedding = queryEmbedding.to(device=device, dtype=dtype)\n",
        "          y = y.to(device=device, dtype=dtype)\n",
        "\n",
        "          scores = model(imageEmbeddings, queryEmbedding)\n",
        "          # print(scores)\n",
        "          # print(y)\n",
        "          output = loss(scores, y)\n",
        "          # print(scores, y)\n",
        "\n",
        "          # Zero out all of the gradients for the variables which the optimizer\n",
        "          # will update.\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # This is the backwards pass: compute the gradient of the loss with\n",
        "          # respect to each  parameter of the model.\n",
        "          output.backward()\n",
        "\n",
        "          # Actually update the parameters of the model using the gradients\n",
        "          # computed by the backwards pass.\n",
        "          optimizer.step()\n",
        "          if scheduler:\n",
        "            scheduler.step()\n",
        "          # print('Iteration %d, loss = %.4f' % (e, output.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3thaF3qkk8aW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = RetrieverNN(imageEmbeddingSize, queryTextEmbeddingSize)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "train_part34(model, optimizer, epochs = 20)\n"
      ],
      "metadata": {
        "id": "HVxih4THo1JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model():\n",
        "#Evaluate model\n",
        "  model.eval()  # set model to evaluation mode\n",
        "  eval_loose = 0\n",
        "  eval_tight = 0\n",
        "  N = len(X_image_eval)\n",
        "  with torch.no_grad():\n",
        "    for i in range(N):\n",
        "      X_image = torch.tensor(X_image_eval[i])\n",
        "      X_query_eval = torch.tensor(X_text_embed_eval[i])\n",
        "      top5 = y_eval[i]\n",
        "      probs = model(X_image, X_query_eval)\n",
        "      top_pred = probs.detach().numpy().squeeze()\n",
        "      # print(prompts[i])\n",
        "      # print(top5[0], top_pred)\n",
        "      pred_idx = np.argmax(top_pred)\n",
        "      if pred_idx in top5:\n",
        "        eval_loose += 1\n",
        "      if pred_idx == top5[0]:\n",
        "        eval_tight += 1\n",
        "  return (float(eval_tight)/ N, float(eval_loose)/N)\n",
        "\n"
      ],
      "metadata": {
        "id": "CuDiH8VwN-_y"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with contrastive examples"
      ],
      "metadata": {
        "id": "5iuZBjJUkRR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_key = \"text_embedding\"\n",
        "image_key = \"image_embeddings_all\"\n",
        "score_key = \"scores\"\n",
        "image_embeddings_top_5 = \"image_embeddings_top5_idx\"\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "y_output = []\n",
        "X_image_eval = []\n",
        "X_text_embed_eval = []\n",
        "y_eval = []\n",
        "prompts = []\n",
        "indeces = []\n",
        "for key, sub_dataset in unwrapped_data.items():\n",
        "  # print(key)\n",
        "  if key[1] != 1:\n",
        "    continue\n",
        "  text_embedding = sub_dataset[text_key]\n",
        "  image_embedding = sub_dataset[image_key]\n",
        "  scores = sub_dataset[score_key]\n",
        "  top5 = sub_dataset[image_embeddings_top_5]\n",
        "  for i in range(1):\n",
        "    idx = top5[i]\n",
        "    # print(top5, len(image_embedding))\n",
        "    if idx >= len(image_embedding):\n",
        "      # print(top5, len(image_embedding))\n",
        "      print(key)\n",
        "      continue\n",
        "    indeces.append(idx)\n",
        "    image_embeddings.append(image_embedding[idx])\n",
        "    text_embeddings.append(text_embedding)\n",
        "    y_output.append(scores[i])\n",
        "  # if top5[0] == 0:\n",
        "  #   continue\n",
        "  X_image_eval.append(image_embedding)\n",
        "  X_text_embed_eval.append([text_embedding] * len(image_embedding))\n",
        "  y_eval.append(top5)\n",
        "  # prompts.append(key)\n",
        "\n",
        "# print(y_output)\n",
        "N = len(image_embeddings)\n",
        "print(len(image_embeddings), len(text_embeddings), len(y_output))\n",
        "# print(y_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQnIByP_tObl",
        "outputId": "cbc12062-d483-4aa6-df23-aa87ea51637b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112 112 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_images = torch.tensor(image_embeddings)\n",
        "X_queries = torch.tensor(text_embeddings)\n",
        "y = torch.tensor(y_output)"
      ],
      "metadata": {
        "id": "_9iCfro8lVNQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.BCELoss()\n",
        "def train_part_contrastive(model, optimizer, epochs=10, scheduler = None):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "      interval = 32\n",
        "      for i in range(0, N, interval):\n",
        "          # Get the current batch and train that batch\n",
        "          image_batch = X_images[i:i + interval].to(device=device, dtype=dtype)\n",
        "          query_batch = X_queries[i:i + interval].to(device=device, dtype=dtype)\n",
        "          y_batch = y[i:i + interval].to(device=device, dtype=dtype)\n",
        "          index_batch = indeces[i:i + interval]\n",
        "          model.train()  # put model to training mode\n",
        "          scores = model(image_batch, query_batch)\n",
        "          output = loss(scores, y_batch)\n",
        "          optimizer.zero_grad()\n",
        "          # output.backward()\n",
        "\n",
        "\n",
        "          # find the best contrastive examples for each text prompt\n",
        "          model.eval()\n",
        "          eval_images = X_image_eval[i:i + interval]\n",
        "          query_eval = X_text_embed_eval[i:i + interval]\n",
        "\n",
        "          contrastive_images = torch.zeros(len(eval_images), imageEmbeddingSize)\n",
        "          for j in range(len(eval_images)):\n",
        "            images_i = torch.tensor(eval_images[j]).to(device=device, dtype=dtype)\n",
        "            queries_i = torch.tensor(query_eval[j]).to(device=device, dtype=dtype)\n",
        "            probs = model(images_i, queries_i).detach().numpy().squeeze()\n",
        "            sorted = np.argsort(probs)[::-1]\n",
        "            # print(index_batch, j)\n",
        "            correct = index_batch[j]\n",
        "\n",
        "            if sorted[0] != correct:\n",
        "              contrastive_images[j] = images_i[sorted[0]]\n",
        "            else:\n",
        "              contrastive_images[j] = images_i[sorted[1]]\n",
        "\n",
        "          # print(contrastive_images)\n",
        "          # image_batch = torch.tensor(contrastive_images).to(device=device, dtype=dtype)\n",
        "          y_batch = torch.zeros(len(eval_images))\n",
        "\n",
        "          model.train()  # put model to training mode\n",
        "          scores = model(contrastive_images, query_batch)\n",
        "          output += loss(scores, y_batch)\n",
        "          output.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          if scheduler:\n",
        "            scheduler.step()\n",
        "      eval_acc = eval_model()\n",
        "      print('Iteration %d, loss = %.4f \\n' % (e, output.item()))\n",
        "      print('Iteration %d, 1_accuracy = %.4f, 5_accuracy = %.4f \\n' % (e, eval_acc[0], eval_acc[1]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YRRk2egtlDES"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RetrieverNN(imageEmbeddingSize, queryTextEmbeddingSize)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "train_part_contrastive(model, optimizer, epochs = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZ7hJNslXus",
        "outputId": "9fde3a79-b931-49f0-9cd6-b46355730166"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 1.3949 \n",
            "\n",
            "Iteration 0, 1_accuracy = 0.1696, 5_accuracy = 0.7857 \n",
            "\n",
            "Iteration 1, loss = 1.3855 \n",
            "\n",
            "Iteration 1, 1_accuracy = 0.1161, 5_accuracy = 0.4107 \n",
            "\n",
            "Iteration 2, loss = 1.3830 \n",
            "\n",
            "Iteration 2, 1_accuracy = 0.1786, 5_accuracy = 0.7500 \n",
            "\n",
            "Iteration 3, loss = 1.3876 \n",
            "\n",
            "Iteration 3, 1_accuracy = 0.1518, 5_accuracy = 0.4732 \n",
            "\n",
            "Iteration 4, loss = 1.3912 \n",
            "\n",
            "Iteration 4, 1_accuracy = 0.1339, 5_accuracy = 0.5179 \n",
            "\n",
            "Iteration 5, loss = 1.3855 \n",
            "\n",
            "Iteration 5, 1_accuracy = 0.2054, 5_accuracy = 0.7411 \n",
            "\n",
            "Iteration 6, loss = 1.3863 \n",
            "\n",
            "Iteration 6, 1_accuracy = 0.1071, 5_accuracy = 0.3214 \n",
            "\n",
            "Iteration 7, loss = 1.3865 \n",
            "\n",
            "Iteration 7, 1_accuracy = 0.0893, 5_accuracy = 0.2946 \n",
            "\n",
            "Iteration 8, loss = 1.3878 \n",
            "\n",
            "Iteration 8, 1_accuracy = 0.0804, 5_accuracy = 0.2143 \n",
            "\n",
            "Iteration 9, loss = 1.3867 \n",
            "\n",
            "Iteration 9, 1_accuracy = 0.0982, 5_accuracy = 0.5357 \n",
            "\n",
            "Iteration 10, loss = 1.3835 \n",
            "\n",
            "Iteration 10, 1_accuracy = 0.2411, 5_accuracy = 0.6429 \n",
            "\n",
            "Iteration 11, loss = 1.3893 \n",
            "\n",
            "Iteration 11, 1_accuracy = 0.1964, 5_accuracy = 0.5357 \n",
            "\n",
            "Iteration 12, loss = 1.3847 \n",
            "\n",
            "Iteration 12, 1_accuracy = 0.1696, 5_accuracy = 0.4018 \n",
            "\n",
            "Iteration 13, loss = 1.3867 \n",
            "\n",
            "Iteration 13, 1_accuracy = 0.2411, 5_accuracy = 0.7411 \n",
            "\n",
            "Iteration 14, loss = 1.3866 \n",
            "\n",
            "Iteration 14, 1_accuracy = 0.2411, 5_accuracy = 0.5804 \n",
            "\n",
            "Iteration 15, loss = 1.3866 \n",
            "\n",
            "Iteration 15, 1_accuracy = 0.2321, 5_accuracy = 0.6339 \n",
            "\n",
            "Iteration 16, loss = 1.3841 \n",
            "\n",
            "Iteration 16, 1_accuracy = 0.1339, 5_accuracy = 0.6161 \n",
            "\n",
            "Iteration 17, loss = 1.3874 \n",
            "\n",
            "Iteration 17, 1_accuracy = 0.1964, 5_accuracy = 0.4286 \n",
            "\n",
            "Iteration 18, loss = 1.3846 \n",
            "\n",
            "Iteration 18, 1_accuracy = 0.2857, 5_accuracy = 0.7143 \n",
            "\n",
            "Iteration 19, loss = 1.3858 \n",
            "\n",
            "Iteration 19, 1_accuracy = 0.3125, 5_accuracy = 0.7321 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XHe-3I4qZfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}