{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "DIRECTORY = \"/content/drive/My Drive/cs231n/final project\"\n",
        "%cd $DIRECTORY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWpbp0N9fkPX",
        "outputId": "49b2c831-838f-4adc-b7d7-ccbbd165139d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/cs231n/final project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFy0pdwMfZ-9",
        "outputId": "44cb5822-e833-41f9-efc1-ec56aefa2a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss.\n",
        "print_every = 100\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imageEmbeddingSize, queryTextEmbeddingSize = 1000, 384"
      ],
      "metadata": {
        "id": "wVssOaGbDPUI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "# Load in data and unwrap it\n",
        "# Function to convert stringified tuple keys back to tuples\n",
        "def unwrap_keys(mapping):\n",
        "    return {literal_eval(k): v for k, v in mapping.items()}\n",
        "\n",
        "# Load the JSON file\n",
        "with open('./embeddings.json', 'r') as json_file:\n",
        "    data_from_json = json.load(json_file)\n",
        "\n",
        "# print(data_from_json)\n",
        "# Unwrap the keys to their original tuple format\n",
        "unwrapped_data = unwrap_keys(data_from_json)"
      ],
      "metadata": {
        "id": "BWelGQatLZ7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_key = \"text_embedding\"\n",
        "image_key = \"image_embeddings_all\"\n",
        "score_key = \"scores\"\n",
        "image_embeddings_top_5 = \"image_embeddings_top5_idx\"\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "y_output = []\n",
        "X_image_eval = []\n",
        "X_text_embed_eval = []\n",
        "y_eval = []\n",
        "prompts = []\n",
        "for key, sub_dataset in unwrapped_data.items():\n",
        "  # pprint.pp(sub_dataset)\n",
        "  text_embedding = sub_dataset[text_key]\n",
        "  image_embedding = sub_dataset[image_key]\n",
        "  # print(len(image_embedding))\n",
        "  scores = sub_dataset[score_key]\n",
        "  top5 = sub_dataset[image_embeddings_top_5]\n",
        "  # print(len(image_embedding), len(text_embedding), len(scores))\n",
        "  # if len(image_embedding) != len(scores):\n",
        "  #   continue\n",
        "  # print(top5, len(image_embedding))\n",
        "  # print(key)\n",
        "  for i in range(1):\n",
        "    idx = top5[i]\n",
        "    # print(top5, len(image_embedding))\n",
        "    if idx >= len(image_embedding):\n",
        "      # print(top5, len(image_embedding))\n",
        "      print(key)\n",
        "      continue\n",
        "    image_embeddings.append(image_embedding[idx])\n",
        "    text_embeddings.append(text_embedding)\n",
        "    y_output.append(scores[i])\n",
        "  if top5[0] == 0:\n",
        "    continue\n",
        "  X_image_eval.append(image_embedding)\n",
        "  X_text_embed_eval.append([text_embedding] * len(image_embedding))\n",
        "  y_eval.append(top5)\n",
        "  prompts.append(key)\n",
        "\n",
        "# print(y_output)\n",
        "N = len(image_embeddings)\n",
        "print(len(image_embeddings), len(text_embeddings), len(y_output))\n",
        "# print(y_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_A5Lsxeq6-w",
        "outputId": "25cccd8e-c9e5-462d-eb29-826e6cb77fc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "224 224 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_images = torch.tensor(image_embeddings)\n",
        "X_queries = torch.tensor(text_embeddings)\n",
        "y = torch.tensor(y_output)\n",
        "print(X_images.shape, X_queries.shape, y.shape)\n",
        "dataset = TensorDataset(X_images, X_queries, y)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# print(X_images.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJI9LC23qRFj",
        "outputId": "4777a836-1832-4214-cb75-7c5fec75e307"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([224, 1000]) torch.Size([224, 384]) torch.Size([224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingProjectionNN(nn.Module):\n",
        "    def __init__(self, embeddingSize):\n",
        "      super().__init__()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(embeddingSize, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(512, 512),\n",
        "          nn.LayerNorm(512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(512, 256),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # input (N, E)\n",
        "      projection = self.linear_relu_stack(x)\n",
        "      return projection\n",
        "\n",
        "class RetrieverNN(nn.Module):\n",
        "  def __init__(self, imageEmbedding, queryEmbedding):\n",
        "    super().__init__()\n",
        "    self.imageProj = EmbeddingProjectionNN(imageEmbedding)\n",
        "    self.queryProj = EmbeddingProjectionNN(queryEmbedding)\n",
        "    self.similarity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, images, query):\n",
        "    imageProj = self.imageProj(images)\n",
        "    queryProj = self.queryProj(query)\n",
        "    similarities = self.similarity(imageProj, queryProj)\n",
        "    # make similarities a prob scores between 0 and 1\n",
        "    scores = similarities * 0.5 + 0.5\n",
        "    return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "kvJJm9zsf2BO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.BCELoss()\n",
        "def train_part34(model, optimizer, epochs=10, scheduler = None):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "\n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for imageEmbeddings, queryEmbedding, y in dataloader:\n",
        "          # (B, E1), (B, E2), (B,)\n",
        "          model.train()  # put model to training mode\n",
        "          imageEmbeddings = imageEmbeddings.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "          queryEmbedding = queryEmbedding.to(device=device, dtype=dtype)\n",
        "          y = y.to(device=device, dtype=dtype)\n",
        "\n",
        "          scores = model(imageEmbeddings, queryEmbedding)\n",
        "          # print(scores)\n",
        "          # print(y)\n",
        "          output = loss(scores, y)\n",
        "          # print(scores, y)\n",
        "\n",
        "          # Zero out all of the gradients for the variables which the optimizer\n",
        "          # will update.\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # This is the backwards pass: compute the gradient of the loss with\n",
        "          # respect to each  parameter of the model.\n",
        "          output.backward()\n",
        "\n",
        "          # Actually update the parameters of the model using the gradients\n",
        "          # computed by the backwards pass.\n",
        "          optimizer.step()\n",
        "          if scheduler:\n",
        "            scheduler.step()\n",
        "          print('Iteration %d, loss = %.4f' % (e, output.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3thaF3qkk8aW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = RetrieverNN(imageEmbeddingSize, queryTextEmbeddingSize)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "train_part34(model, optimizer, epochs = 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVxih4THo1JC",
        "outputId": "15f5c063-321a-4aeb-b05b-daf626540a91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 0.6924\n",
            "Iteration 0, loss = 0.7081\n",
            "Iteration 0, loss = 0.8779\n",
            "Iteration 0, loss = 1.0768\n",
            "Iteration 0, loss = 0.6839\n",
            "Iteration 0, loss = 0.7129\n",
            "Iteration 0, loss = 0.6876\n",
            "Iteration 1, loss = 0.6651\n",
            "Iteration 1, loss = 0.7220\n",
            "Iteration 1, loss = 0.6787\n",
            "Iteration 1, loss = 0.6998\n",
            "Iteration 1, loss = 0.6953\n",
            "Iteration 1, loss = 0.6884\n",
            "Iteration 1, loss = 0.6964\n",
            "Iteration 2, loss = 0.6853\n",
            "Iteration 2, loss = 0.6773\n",
            "Iteration 2, loss = 0.6773\n",
            "Iteration 2, loss = 0.6798\n",
            "Iteration 2, loss = 0.6646\n",
            "Iteration 2, loss = 0.6637\n",
            "Iteration 2, loss = 0.6764\n",
            "Iteration 3, loss = 0.6923\n",
            "Iteration 3, loss = 0.6573\n",
            "Iteration 3, loss = 0.6540\n",
            "Iteration 3, loss = 0.6637\n",
            "Iteration 3, loss = 0.6488\n",
            "Iteration 3, loss = 0.6541\n",
            "Iteration 3, loss = 0.6153\n",
            "Iteration 4, loss = 0.6426\n",
            "Iteration 4, loss = 0.6075\n",
            "Iteration 4, loss = 0.6134\n",
            "Iteration 4, loss = 0.6127\n",
            "Iteration 4, loss = 0.6394\n",
            "Iteration 4, loss = 0.6108\n",
            "Iteration 4, loss = 0.6207\n",
            "Iteration 5, loss = 0.6216\n",
            "Iteration 5, loss = 0.5922\n",
            "Iteration 5, loss = 0.6080\n",
            "Iteration 5, loss = 0.6232\n",
            "Iteration 5, loss = 0.5274\n",
            "Iteration 5, loss = 0.5550\n",
            "Iteration 5, loss = 0.5586\n",
            "Iteration 6, loss = 0.5522\n",
            "Iteration 6, loss = 0.5424\n",
            "Iteration 6, loss = 0.4967\n",
            "Iteration 6, loss = 0.6113\n",
            "Iteration 6, loss = 0.5396\n",
            "Iteration 6, loss = 0.5036\n",
            "Iteration 6, loss = 0.5005\n",
            "Iteration 7, loss = 0.4407\n",
            "Iteration 7, loss = 0.4302\n",
            "Iteration 7, loss = 0.5477\n",
            "Iteration 7, loss = 0.4471\n",
            "Iteration 7, loss = 0.4564\n",
            "Iteration 7, loss = 0.3907\n",
            "Iteration 7, loss = 0.5078\n",
            "Iteration 8, loss = 0.3782\n",
            "Iteration 8, loss = 0.5769\n",
            "Iteration 8, loss = 0.4295\n",
            "Iteration 8, loss = 0.3215\n",
            "Iteration 8, loss = 0.2919\n",
            "Iteration 8, loss = 0.5097\n",
            "Iteration 8, loss = 0.2644\n",
            "Iteration 9, loss = 0.4288\n",
            "Iteration 9, loss = 0.4089\n",
            "Iteration 9, loss = 0.4915\n",
            "Iteration 9, loss = 0.5333\n",
            "Iteration 9, loss = 0.3307\n",
            "Iteration 9, loss = 0.5239\n",
            "Iteration 9, loss = 0.4576\n",
            "Iteration 10, loss = 0.4360\n",
            "Iteration 10, loss = 0.2330\n",
            "Iteration 10, loss = 0.4218\n",
            "Iteration 10, loss = 0.3747\n",
            "Iteration 10, loss = 0.3546\n",
            "Iteration 10, loss = 0.5456\n",
            "Iteration 10, loss = 0.3927\n",
            "Iteration 11, loss = 0.2795\n",
            "Iteration 11, loss = 0.5156\n",
            "Iteration 11, loss = 0.3295\n",
            "Iteration 11, loss = 0.3613\n",
            "Iteration 11, loss = 0.3189\n",
            "Iteration 11, loss = 0.4703\n",
            "Iteration 11, loss = 0.3997\n",
            "Iteration 12, loss = 0.3999\n",
            "Iteration 12, loss = 0.2789\n",
            "Iteration 12, loss = 0.5387\n",
            "Iteration 12, loss = 0.3601\n",
            "Iteration 12, loss = 0.4773\n",
            "Iteration 12, loss = 0.4580\n",
            "Iteration 12, loss = 0.3739\n",
            "Iteration 13, loss = 0.3870\n",
            "Iteration 13, loss = 0.2921\n",
            "Iteration 13, loss = 0.5153\n",
            "Iteration 13, loss = 0.3055\n",
            "Iteration 13, loss = 0.2335\n",
            "Iteration 13, loss = 0.4685\n",
            "Iteration 13, loss = 0.4454\n",
            "Iteration 14, loss = 0.3207\n",
            "Iteration 14, loss = 0.4029\n",
            "Iteration 14, loss = 0.3984\n",
            "Iteration 14, loss = 0.2733\n",
            "Iteration 14, loss = 0.2624\n",
            "Iteration 14, loss = 0.3604\n",
            "Iteration 14, loss = 0.4241\n",
            "Iteration 15, loss = 0.4684\n",
            "Iteration 15, loss = 0.3920\n",
            "Iteration 15, loss = 0.4159\n",
            "Iteration 15, loss = 0.1957\n",
            "Iteration 15, loss = 0.4158\n",
            "Iteration 15, loss = 0.3107\n",
            "Iteration 15, loss = 0.2511\n",
            "Iteration 16, loss = 0.3958\n",
            "Iteration 16, loss = 0.2732\n",
            "Iteration 16, loss = 0.3575\n",
            "Iteration 16, loss = 0.2793\n",
            "Iteration 16, loss = 0.2833\n",
            "Iteration 16, loss = 0.2351\n",
            "Iteration 16, loss = 0.2693\n",
            "Iteration 17, loss = 0.3176\n",
            "Iteration 17, loss = 0.2645\n",
            "Iteration 17, loss = 0.2141\n",
            "Iteration 17, loss = 0.3118\n",
            "Iteration 17, loss = 0.3096\n",
            "Iteration 17, loss = 0.2890\n",
            "Iteration 17, loss = 0.3136\n",
            "Iteration 18, loss = 0.3806\n",
            "Iteration 18, loss = 0.3602\n",
            "Iteration 18, loss = 0.3341\n",
            "Iteration 18, loss = 0.3743\n",
            "Iteration 18, loss = 0.2106\n",
            "Iteration 18, loss = 0.3430\n",
            "Iteration 18, loss = 0.1489\n",
            "Iteration 19, loss = 0.2158\n",
            "Iteration 19, loss = 0.3138\n",
            "Iteration 19, loss = 0.2860\n",
            "Iteration 19, loss = 0.3950\n",
            "Iteration 19, loss = 0.2784\n",
            "Iteration 19, loss = 0.2297\n",
            "Iteration 19, loss = 0.2813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # set model to evaluation mode\n",
        "eval_loose = 0\n",
        "eval_tight = 0\n",
        "N = len(X_image_eval)\n",
        "with torch.no_grad():\n",
        "  for i in range(N):\n",
        "    X_image = torch.tensor(X_image_eval[i])\n",
        "    X_query_eval = torch.tensor(X_text_embed_eval[i])\n",
        "    top5 = y_eval[i]\n",
        "    probs = model(X_image, X_query_eval)\n",
        "    top_pred = probs.detach().numpy().squeeze()\n",
        "    # print(prompts[i])\n",
        "    print(top5[0], top_pred)\n",
        "    pred_idx = np.argmax(top_pred)\n",
        "    if pred_idx in top5:\n",
        "      eval_loose += 1\n",
        "    if pred_idx == top5[0]:\n",
        "      eval_tight += 1\n",
        "print(float(eval_tight)/ N, float(eval_loose)/N)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuDiH8VwN-_y",
        "outputId": "ec0ae1bf-01b7-4e4f-96f9-0f657ec4962d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 [0.7594562  0.96876264 0.9687709  0.9561261  0.49438563 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646883 0.67671984 0.42568076\n",
            " 0.33498877 0.3527168  0.6985896  0.44032323 0.4178658  0.9350209\n",
            " 0.7201774  0.8628553 ]\n",
            "1 [0.7594563  0.96876264 0.96877086 0.95612615 0.49438563 0.67538357\n",
            " 0.96875226 0.3707183  0.96875995 0.4764688  0.67671984 0.42568076\n",
            " 0.3349887  0.35271674 0.6985896  0.44032323 0.4178658  0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "7 [0.7594563  0.96876264 0.96877086 0.95612615 0.49438563 0.67538357\n",
            " 0.96875226 0.3707183  0.96875995 0.4764688  0.67671984 0.42568076\n",
            " 0.3349887  0.35271674 0.6985896  0.44032323 0.4178658  0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "2 [0.7594562  0.96876264 0.96877086 0.95612615 0.49438563 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646883 0.6767198  0.42568076\n",
            " 0.33498874 0.3527168  0.69858956 0.44032323 0.41786584 0.9350209\n",
            " 0.7201774  0.8628553 ]\n",
            "7 [0.7594562  0.96876264 0.96877086 0.95612615 0.49438563 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646883 0.6767198  0.42568076\n",
            " 0.33498874 0.3527168  0.69858956 0.44032323 0.41786584 0.9350209\n",
            " 0.7201774  0.8628553 ]\n",
            "3 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.3527168  0.6985897  0.44032326 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "7 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.3527168  0.6985897  0.44032326 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "3 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.3527168  0.6985897  0.44032323 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "7 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.67538357\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.3527168  0.6985897  0.44032323 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "3 [0.75945616 0.96876264 0.96877086 0.9561261  0.4943856  0.67538357\n",
            " 0.9687522  0.37071836 0.96875995 0.4764688  0.6767198  0.42568076\n",
            " 0.33498877 0.3527168  0.69858956 0.44032323 0.41786584 0.9350209\n",
            " 0.7201774  0.8628553 ]\n",
            "12 [0.75945616 0.96876264 0.96877086 0.9561261  0.4943856  0.67538357\n",
            " 0.9687522  0.37071836 0.96875995 0.4764688  0.6767198  0.42568076\n",
            " 0.33498877 0.3527168  0.69858956 0.44032323 0.41786584 0.9350209\n",
            " 0.7201774  0.8628553 ]\n",
            "3 [0.7594562  0.96876264 0.9687709  0.9561262  0.49438563 0.67538357\n",
            " 0.9687522  0.3707183  0.96875995 0.4764688  0.67671984 0.42568076\n",
            " 0.33498877 0.35271677 0.6985896  0.44032323 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "13 [0.7594562  0.96876264 0.9687709  0.9561262  0.49438563 0.67538357\n",
            " 0.9687522  0.3707183  0.96875995 0.4764688  0.67671984 0.42568076\n",
            " 0.33498877 0.35271677 0.6985896  0.44032323 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "13 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.6753836\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.35271677 0.6985897  0.44032326 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "3 [0.7594562  0.9687626  0.96877086 0.9561261  0.49438566 0.6753836\n",
            " 0.96875215 0.3707183  0.9687599  0.47646886 0.67671984 0.42568076\n",
            " 0.3349887  0.35271677 0.6985897  0.44032326 0.41786584 0.9350209\n",
            " 0.7201775  0.8628553 ]\n",
            "7 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687668\n",
            " 0.96875733 0.3926058  0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "2 [0.4670664  0.96876854 0.96877444 0.9687731  0.96877515 0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "11 [0.4670664  0.96876854 0.96877444 0.9687731  0.96877515 0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "5 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "11 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "6 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687668\n",
            " 0.9687574  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "11 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687668\n",
            " 0.9687574  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "6 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687753  0.96876675\n",
            " 0.96875733 0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "11 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687753  0.96876675\n",
            " 0.96875733 0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "6 [0.46706638 0.96876854 0.96877456 0.96877307 0.9687753  0.96876675\n",
            " 0.9687573  0.39260584 0.50826585 0.47058365 0.53899246 0.02350989]\n",
            "11 [0.46706638 0.96876854 0.96877456 0.96877307 0.9687753  0.96876675\n",
            " 0.9687573  0.39260584 0.50826585 0.47058365 0.53899246 0.02350989]\n",
            "6 [0.4670664  0.9687686  0.96877444 0.9687731  0.9687753  0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.53899246 0.02350992]\n",
            "11 [0.4670664  0.9687686  0.96877444 0.9687731  0.9687753  0.9687667\n",
            " 0.9687573  0.39260584 0.5082659  0.47058368 0.53899246 0.02350992]\n",
            "10 [0.4670664  0.9687686  0.9687745  0.9687731  0.9687752  0.9687668\n",
            " 0.9687574  0.39260584 0.5082659  0.47058368 0.5389925  0.02350992]\n",
            "11 [0.47018686 0.96875846 0.9686702  0.96875644 0.967583   0.96877193\n",
            " 0.9684324  0.8348679  0.5158186  0.3016465  0.02350992 0.02350992]\n",
            "1 [0.47018683 0.96875846 0.9686701  0.96875644 0.96758294 0.96877193\n",
            " 0.9684324  0.83486795 0.51581854 0.3016465  0.02350992 0.02350992]\n",
            "11 [0.47018683 0.96875846 0.9686701  0.96875644 0.96758294 0.96877193\n",
            " 0.9684324  0.83486795 0.51581854 0.3016465  0.02350992 0.02350992]\n",
            "2 [0.47018683 0.96875846 0.9686702  0.9687564  0.96758294 0.96877193\n",
            " 0.9684324  0.8348679  0.51581854 0.30164653 0.02350995 0.02350995]\n",
            "11 [0.47018683 0.96875846 0.9686702  0.9687564  0.96758294 0.96877193\n",
            " 0.9684324  0.8348679  0.51581854 0.30164653 0.02350995 0.02350995]\n",
            "3 [0.47018686 0.96875846 0.9686701  0.96875644 0.96758294 0.968772\n",
            " 0.9684324  0.83486795 0.5158186  0.3016465  0.02350992 0.02350992]\n",
            "11 [0.47018686 0.96875846 0.9686701  0.96875644 0.96758294 0.968772\n",
            " 0.9684324  0.83486795 0.5158186  0.3016465  0.02350992 0.02350992]\n",
            "4 [0.47018686 0.96875846 0.9686702  0.96875644 0.96758306 0.968772\n",
            " 0.9684324  0.83486795 0.5158186  0.30164653 0.02350995 0.02350995]\n",
            "11 [0.47018686 0.96875846 0.9686702  0.96875644 0.96758306 0.968772\n",
            " 0.9684324  0.83486795 0.5158186  0.30164653 0.02350995 0.02350995]\n",
            "3 [0.47018683 0.96875846 0.9686702  0.96875644 0.96758306 0.96877193\n",
            " 0.9684324  0.83486795 0.5158186  0.3016465  0.02350995 0.02350995]\n",
            "11 [0.47018683 0.96875846 0.9686702  0.96875644 0.96758306 0.96877193\n",
            " 0.9684324  0.83486795 0.5158186  0.3016465  0.02350995 0.02350995]\n",
            "4 [0.47018683 0.9687584  0.9686702  0.96875644 0.96758294 0.96877193\n",
            " 0.9684324  0.8348679  0.51581854 0.30164653 0.02350995 0.02350995]\n",
            "11 [0.47018683 0.9687584  0.9686702  0.96875644 0.96758294 0.96877193\n",
            " 0.9684324  0.8348679  0.51581854 0.30164653 0.02350995 0.02350995]\n",
            "9 [0.47018686 0.96875846 0.9686702  0.96875644 0.96758306 0.968772\n",
            " 0.9684324  0.83486795 0.5158186  0.3016465  0.02350992 0.02350992]\n",
            "19 [0.5644067  0.96876156 0.9687748  0.96814257 0.9667081  0.7208464\n",
            " 0.7863479  0.9426888  0.87370527 0.65296596 0.9687612  0.51690614\n",
            " 0.8484644  0.5221202  0.3284588  0.39588422 0.8773282  0.64071506\n",
            " 0.3361585  0.23628646]\n",
            "1 [0.56440663 0.9687615  0.9687748  0.9681426  0.9667081  0.7208464\n",
            " 0.7863479  0.9426889  0.87370527 0.65296596 0.9687612  0.51690614\n",
            " 0.84846437 0.5221202  0.32845876 0.3958842  0.8773283  0.64071506\n",
            " 0.33615845 0.23628643]\n",
            "19 [0.56440663 0.9687615  0.9687748  0.9681426  0.9667081  0.7208464\n",
            " 0.7863479  0.9426889  0.87370527 0.65296596 0.9687612  0.51690614\n",
            " 0.84846437 0.5221202  0.32845876 0.3958842  0.8773283  0.64071506\n",
            " 0.33615845 0.23628643]\n",
            "2 [0.5644067  0.96876156 0.9687748  0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426888  0.87370527 0.6529659  0.9687612  0.51690614\n",
            " 0.8484644  0.5221202  0.3284588  0.39588422 0.8773283  0.64071506\n",
            " 0.3361585  0.23628652]\n",
            "19 [0.5644067  0.96876156 0.9687748  0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426888  0.87370527 0.6529659  0.9687612  0.51690614\n",
            " 0.8484644  0.5221202  0.3284588  0.39588422 0.8773283  0.64071506\n",
            " 0.3361585  0.23628652]\n",
            "3 [0.5644067  0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.652966   0.9687612  0.5169062\n",
            " 0.8484645  0.52212024 0.3284588  0.39588422 0.8773283  0.64071506\n",
            " 0.33615848 0.23628646]\n",
            "19 [0.5644067  0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.652966   0.9687612  0.5169062\n",
            " 0.8484645  0.52212024 0.3284588  0.39588422 0.8773283  0.64071506\n",
            " 0.33615848 0.23628646]\n",
            "4 [0.5644067  0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.652966   0.9687612  0.5169062\n",
            " 0.8484645  0.52212024 0.3284588  0.39588422 0.8773283  0.6407151\n",
            " 0.33615848 0.23628646]\n",
            "19 [0.5644067  0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.652966   0.9687612  0.5169062\n",
            " 0.8484645  0.52212024 0.3284588  0.39588422 0.8773283  0.6407151\n",
            " 0.33615848 0.23628646]\n",
            "3 [0.56440663 0.9687615  0.96877486 0.96814257 0.96670806 0.7208464\n",
            " 0.7863479  0.9426889  0.87370527 0.6529659  0.9687612  0.51690614\n",
            " 0.84846437 0.5221202  0.32845885 0.39588428 0.8773283  0.640715\n",
            " 0.33615854 0.23628652]\n",
            "19 [0.56440663 0.9687615  0.96877486 0.96814257 0.96670806 0.7208464\n",
            " 0.7863479  0.9426889  0.87370527 0.6529659  0.9687612  0.51690614\n",
            " 0.84846437 0.5221202  0.32845885 0.39588428 0.8773283  0.640715\n",
            " 0.33615854 0.23628652]\n",
            "4 [0.56440663 0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.65296596 0.9687612  0.51690614\n",
            " 0.8484644  0.5221202  0.32845882 0.39588422 0.8773283  0.64071506\n",
            " 0.3361585  0.23628646]\n",
            "19 [0.56440663 0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.65296596 0.9687612  0.51690614\n",
            " 0.8484644  0.5221202  0.32845882 0.39588422 0.8773283  0.64071506\n",
            " 0.3361585  0.23628646]\n",
            "19 [0.5644067  0.96876156 0.96877486 0.96814257 0.9667082  0.7208464\n",
            " 0.786348   0.9426889  0.8737053  0.652966   0.9687612  0.5169062\n",
            " 0.8484645  0.52212024 0.3284588  0.39588422 0.8773283  0.6407151\n",
            " 0.33615848 0.23628646]\n",
            "15 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.8537791  0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992211  0.02350992]\n",
            "1 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.96875894\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.8865629\n",
            " 0.7567983  0.82762843 0.8992211  0.02350992]\n",
            "15 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.96875894\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.8865629\n",
            " 0.7567983  0.82762843 0.8992211  0.02350992]\n",
            "5 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.8537791  0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992211  0.02350992]\n",
            "15 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.8537791  0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992211  0.02350992]\n",
            "6 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.9280303  0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "15 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.9280303  0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "7 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "15 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "6 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.89922106 0.02350992]\n",
            "15 [0.70189047 0.96876717 0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.89922106 0.02350992]\n",
            "7 [0.7018904  0.96876717 0.96877444 0.9687713  0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.57253957 0.5345853  0.88656294\n",
            " 0.7567983  0.8276285  0.89922106 0.02350992]\n",
            "15 [0.7018904  0.96876717 0.96877444 0.9687713  0.9687765  0.968759\n",
            " 0.92803025 0.853779   0.7915077  0.57253957 0.5345853  0.88656294\n",
            " 0.7567983  0.8276285  0.89922106 0.02350992]\n",
            "14 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.9280303  0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "6 [0.70189047 0.9687672  0.9687745  0.96877134 0.9687765  0.968759\n",
            " 0.9280303  0.853779   0.7915077  0.5725396  0.5345853  0.88656294\n",
            " 0.7567984  0.8276285  0.8992212  0.02350992]\n",
            "11 [0.30478847 0.9687617  0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.79233545 0.71076226 0.18975556]\n",
            "1 [0.30478847 0.9687617  0.962446   0.9658594  0.8899081  0.6272562\n",
            " 0.4151281  0.90521455 0.68371224 0.79233545 0.71076226 0.18975559]\n",
            "11 [0.30478847 0.9687617  0.962446   0.9658594  0.8899081  0.6272562\n",
            " 0.4151281  0.90521455 0.68371224 0.79233545 0.71076226 0.18975559]\n",
            "2 [0.30478847 0.96876174 0.962446   0.9658594  0.8899081  0.62725616\n",
            " 0.41512814 0.90521455 0.68371224 0.7923354  0.71076226 0.18975556]\n",
            "11 [0.30478847 0.96876174 0.962446   0.9658594  0.8899081  0.62725616\n",
            " 0.41512814 0.90521455 0.68371224 0.7923354  0.71076226 0.18975556]\n",
            "3 [0.30478847 0.96876174 0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.7923355  0.71076226 0.18975556]\n",
            "11 [0.30478847 0.96876174 0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.7923355  0.71076226 0.18975556]\n",
            "4 [0.30478844 0.9687618  0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.7923355  0.71076226 0.18975556]\n",
            "11 [0.30478844 0.9687618  0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.7923355  0.71076226 0.18975556]\n",
            "3 [0.30478847 0.96876174 0.962446   0.9658594  0.889908   0.62725616\n",
            " 0.41512814 0.90521455 0.68371224 0.79233545 0.71076226 0.18975556]\n",
            "11 [0.30478847 0.96876174 0.962446   0.9658594  0.889908   0.62725616\n",
            " 0.41512814 0.90521455 0.68371224 0.79233545 0.71076226 0.18975556]\n",
            "4 [0.30478847 0.9687618  0.962446   0.9658595  0.8899081  0.62725616\n",
            " 0.41512814 0.9052146  0.68371224 0.79233545 0.71076226 0.18975556]\n",
            "11 [0.30478847 0.9687618  0.962446   0.9658595  0.8899081  0.62725616\n",
            " 0.41512814 0.9052146  0.68371224 0.79233545 0.71076226 0.18975556]\n",
            "11 [0.30478847 0.96876174 0.962446   0.9658595  0.8899081  0.6272562\n",
            " 0.41512814 0.90521455 0.68371224 0.7923355  0.71076226 0.18975556]\n",
            "23 [0.6759586  0.96875787 0.96874654 0.96673185 0.7796573  0.7740309\n",
            " 0.9111881  0.6996211  0.94918007 0.7046077  0.73113847 0.9545641\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.80746585 0.8858735  0.7022804  0.20862532]\n",
            "1 [0.6759585  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.911188   0.699621   0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.8592785  0.8726087  0.86143416 0.82008326\n",
            " 0.91895527 0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "23 [0.6759585  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.911188   0.699621   0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.8592785  0.8726087  0.86143416 0.82008326\n",
            " 0.91895527 0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "2 [0.6759585  0.96875787 0.96874654 0.9667318  0.7796573  0.77403086\n",
            " 0.9111881  0.6996211  0.94918    0.7046077  0.73113847 0.9545641\n",
            " 0.58766913 0.755947   0.85927856 0.8726087  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.80746585 0.88587344 0.7022804  0.20862532]\n",
            "23 [0.6759585  0.96875787 0.96874654 0.9667318  0.7796573  0.77403086\n",
            " 0.9111881  0.6996211  0.94918    0.7046077  0.73113847 0.9545641\n",
            " 0.58766913 0.755947   0.85927856 0.8726087  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.80746585 0.88587344 0.7022804  0.20862532]\n",
            "3 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "23 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "4 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.8200832\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "23 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.8200832\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "3 [0.6759585  0.96875787 0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.9111881  0.699621   0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726087  0.86143416 0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "23 [0.6759585  0.96875787 0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.9111881  0.699621   0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726087  0.86143416 0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "4 [0.6759585  0.9687578  0.9687465  0.9667318  0.77965724 0.77403086\n",
            " 0.911188   0.699621   0.94918007 0.7046077  0.7311384  0.95456403\n",
            " 0.58766913 0.755947   0.8592785  0.8726087  0.86143416 0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "23 [0.6759585  0.9687578  0.9687465  0.9667318  0.77965724 0.77403086\n",
            " 0.911188   0.699621   0.94918007 0.7046077  0.7311384  0.95456403\n",
            " 0.58766913 0.755947   0.8592785  0.8726087  0.86143416 0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862532]\n",
            "23 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "4 [0.6759586  0.9687578  0.96874654 0.9667318  0.77965724 0.77403086\n",
            " 0.91118807 0.6996211  0.94918    0.7046077  0.73113847 0.95456403\n",
            " 0.58766913 0.755947   0.85927856 0.8726088  0.8614342  0.82008326\n",
            " 0.9189553  0.501816   0.8074658  0.88587344 0.7022804  0.20862538]\n",
            "15 [0.71821856 0.96875226 0.37336054 0.88003206 0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.90029085 0.85638213 0.9347844\n",
            " 0.9686038  0.02350989 0.02350989 0.02350989]\n",
            "1 [0.7182185  0.96875226 0.3733605  0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.9002908  0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "15 [0.7182185  0.96875226 0.3733605  0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.9002908  0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "1 [0.7182185  0.96875226 0.37336054 0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.9002908  0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "15 [0.7182185  0.96875226 0.37336054 0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.9002908  0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "2 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.85711044 0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "15 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.85711044 0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "3 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.85711044 0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "15 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.85711044 0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350992 0.02350992 0.02350992]\n",
            "2 [0.7182185  0.96875226 0.37336057 0.88003194 0.9536662  0.7407413\n",
            " 0.9685109  0.85711044 0.8775829  0.90029085 0.8563821  0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "15 [0.7182185  0.96875226 0.37336057 0.88003194 0.9536662  0.7407413\n",
            " 0.9685109  0.85711044 0.8775829  0.90029085 0.8563821  0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "3 [0.7182185  0.9687522  0.37336054 0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350995 0.02350995 0.02350995]\n",
            "15 [0.7182185  0.9687522  0.37336054 0.880032   0.9536662  0.7407414\n",
            " 0.96851087 0.85711044 0.8775829  0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350995 0.02350995 0.02350995]\n",
            "12 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.8571105  0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "2 [0.71821856 0.96875226 0.3733605  0.88003206 0.9536662  0.7407414\n",
            " 0.9685109  0.8571105  0.87758297 0.90029085 0.85638213 0.9347844\n",
            " 0.96860385 0.02350989 0.02350989 0.02350989]\n",
            "11 [0.40181646 0.96876645 0.9684274  0.93706405 0.7600007  0.8313927\n",
            " 0.93573356 0.9685331  0.79930896 0.44222468 0.5848191  0.02350992]\n",
            "1 [0.4018164  0.96876645 0.96842754 0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.96853316 0.79930896 0.44222462 0.5848191  0.02350983]\n",
            "11 [0.4018164  0.96876645 0.96842754 0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.96853316 0.79930896 0.44222462 0.5848191  0.02350983]\n",
            "1 [0.40181643 0.96876645 0.9684275  0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.9685331  0.79930896 0.44222465 0.5848191  0.02350989]\n",
            "11 [0.40181643 0.96876645 0.9684275  0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.9685331  0.79930896 0.44222465 0.5848191  0.02350989]\n",
            "2 [0.40181643 0.96876645 0.9684274  0.93706405 0.7600007  0.8313927\n",
            " 0.93573356 0.9685331  0.799309   0.44222468 0.5848191  0.02350992]\n",
            "11 [0.40181643 0.96876645 0.9684274  0.93706405 0.7600007  0.8313927\n",
            " 0.93573356 0.9685331  0.799309   0.44222468 0.5848191  0.02350992]\n",
            "3 [0.40181643 0.96876645 0.9684274  0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.9685331  0.799309   0.44222468 0.5848191  0.02350992]\n",
            "11 [0.40181643 0.96876645 0.9684274  0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.9685331  0.799309   0.44222468 0.5848191  0.02350992]\n",
            "2 [0.40181646 0.96876645 0.9684275  0.93706405 0.7600007  0.83139265\n",
            " 0.93573356 0.96853316 0.7993089  0.44222465 0.584819   0.02350983]\n",
            "11 [0.40181646 0.96876645 0.9684275  0.93706405 0.7600007  0.83139265\n",
            " 0.93573356 0.96853316 0.7993089  0.44222465 0.584819   0.02350983]\n",
            "3 [0.40181643 0.96876645 0.9684275  0.93706405 0.7600007  0.8313927\n",
            " 0.93573356 0.9685331  0.79930896 0.44222465 0.5848191  0.02350989]\n",
            "11 [0.40181643 0.96876645 0.9684275  0.93706405 0.7600007  0.8313927\n",
            " 0.93573356 0.9685331  0.79930896 0.44222465 0.5848191  0.02350989]\n",
            "10 [0.40181643 0.96876645 0.9684274  0.93706405 0.7600007  0.83139277\n",
            " 0.93573356 0.9685331  0.799309   0.44222468 0.5848191  0.02350992]\n",
            "11 [0.43835056 0.9687635  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.84649694 0.3421797  0.320728   0.40612155 0.02350989 0.02350989]\n",
            "1 [0.4383505  0.9687635  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.84649694 0.34217963 0.3207279  0.4061215  0.02350983 0.02350983]\n",
            "11 [0.4383505  0.9687635  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.84649694 0.34217963 0.3207279  0.4061215  0.02350983 0.02350983]\n",
            "1 [0.4383505  0.9687634  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.8464969  0.34217966 0.32072797 0.40612155 0.02350989 0.02350989]\n",
            "11 [0.4383505  0.9687634  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.8464969  0.34217966 0.32072797 0.40612155 0.02350989 0.02350989]\n",
            "2 [0.43835053 0.9687634  0.96875894 0.9471101  0.8959371  0.930484\n",
            " 0.84649694 0.3421797  0.32072797 0.40612155 0.02350995 0.02350995]\n",
            "11 [0.43835053 0.9687634  0.96875894 0.9471101  0.8959371  0.930484\n",
            " 0.84649694 0.3421797  0.32072797 0.40612155 0.02350995 0.02350995]\n",
            "2 [0.43835053 0.9687634  0.96875894 0.9471101  0.8959371  0.930484\n",
            " 0.84649694 0.34217966 0.32072794 0.40612155 0.02350995 0.02350995]\n",
            "11 [0.43835053 0.9687634  0.96875894 0.9471101  0.8959371  0.930484\n",
            " 0.84649694 0.34217966 0.32072794 0.40612155 0.02350995 0.02350995]\n",
            "2 [0.4383505  0.9687635  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.84649694 0.3421797  0.32072794 0.40612155 0.02350983 0.02350983]\n",
            "11 [0.4383505  0.9687635  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.84649694 0.3421797  0.32072794 0.40612155 0.02350983 0.02350983]\n",
            "2 [0.4383505  0.9687634  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.8464969  0.34217966 0.32072794 0.40612155 0.02350989 0.02350989]\n",
            "11 [0.4383505  0.9687634  0.96875894 0.9471102  0.8959371  0.93048406\n",
            " 0.8464969  0.34217966 0.32072794 0.40612155 0.02350989 0.02350989]\n",
            "9 [0.43835053 0.9687634  0.96875894 0.9471101  0.8959371  0.930484\n",
            " 0.84649694 0.3421797  0.32072794 0.40612155 0.02350992 0.02350992]\n",
            "7 [0.43877587 0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391094 0.26408356 0.02350983]\n",
            "1 [0.43877587 0.96876025 0.968763   0.9687687  0.9680667  0.55776155\n",
            " 0.45767003 0.33248866 0.6659763  0.24391091 0.2640835  0.02350989]\n",
            "11 [0.43877587 0.96876025 0.968763   0.9687687  0.9680667  0.55776155\n",
            " 0.45767003 0.33248866 0.6659763  0.24391091 0.2640835  0.02350989]\n",
            "1 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391094 0.26408356 0.02350986]\n",
            "10 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391094 0.26408356 0.02350986]\n",
            "2 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.66597635 0.24391091 0.26408353 0.0235098 ]\n",
            "11 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.66597635 0.24391091 0.26408353 0.0235098 ]\n",
            "2 [0.4387759  0.9687604  0.968763   0.9687688  0.96806675 0.5577616\n",
            " 0.4576701  0.33248872 0.66597635 0.24391091 0.2640835  0.0235098 ]\n",
            "10 [0.4387759  0.9687604  0.968763   0.9687688  0.96806675 0.5577616\n",
            " 0.4576701  0.33248872 0.66597635 0.24391091 0.2640835  0.0235098 ]\n",
            "2 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391091 0.26408356 0.02350983]\n",
            "9 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391091 0.26408356 0.02350983]\n",
            "2 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391094 0.26408356 0.02350986]\n",
            "11 [0.4387759  0.9687603  0.968763   0.9687688  0.9680667  0.5577616\n",
            " 0.45767006 0.33248872 0.6659763  0.24391094 0.26408356 0.02350986]\n",
            "10 [0.4387759  0.9687603  0.968763   0.9687688  0.96806675 0.55776167\n",
            " 0.45767006 0.33248872 0.66597635 0.24391091 0.2640835  0.0235098 ]\n",
            "8 [0.42684036 0.9687635  0.9687421  0.91864324 0.65254915 0.23873937\n",
            " 0.12476125 0.22765237 0.12635851 0.33061993 0.51543796 0.02350992]\n",
            "1 [0.4268403  0.9687635  0.968742   0.9186432  0.6525489  0.23873928\n",
            " 0.12476128 0.22765231 0.12635851 0.33061987 0.5154377  0.02350989]\n",
            "8 [0.4268403  0.9687635  0.968742   0.9186432  0.6525489  0.23873928\n",
            " 0.12476128 0.22765231 0.12635851 0.33061987 0.5154377  0.02350989]\n",
            "2 [0.42684036 0.9687635  0.968742   0.9186432  0.65254897 0.23873934\n",
            " 0.12476125 0.2276524  0.12635851 0.33061993 0.5154378  0.02350983]\n",
            "10 [0.42684036 0.9687635  0.968742   0.9186432  0.65254897 0.23873934\n",
            " 0.12476125 0.2276524  0.12635851 0.33061993 0.5154378  0.02350983]\n",
            "3 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873931\n",
            " 0.12476125 0.22765237 0.12635851 0.33061993 0.51543784 0.02350983]\n",
            "8 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873931\n",
            " 0.12476125 0.22765237 0.12635851 0.33061993 0.51543784 0.02350983]\n",
            "3 [0.42684036 0.96876353 0.9687421  0.91864324 0.652549   0.23873931\n",
            " 0.12476122 0.22765237 0.12635851 0.33061993 0.51543784 0.0235098 ]\n",
            "7 [0.42684036 0.96876353 0.9687421  0.91864324 0.652549   0.23873931\n",
            " 0.12476122 0.22765237 0.12635851 0.33061993 0.51543784 0.0235098 ]\n",
            "3 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873937\n",
            " 0.12476125 0.2276524  0.12635848 0.33061993 0.5154378  0.02350983]\n",
            "8 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873937\n",
            " 0.12476125 0.2276524  0.12635848 0.33061993 0.5154378  0.02350983]\n",
            "3 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873934\n",
            " 0.12476122 0.22765237 0.12635851 0.33061993 0.5154378  0.02350986]\n",
            "8 [0.42684036 0.96876353 0.968742   0.91864324 0.652549   0.23873934\n",
            " 0.12476122 0.22765237 0.12635851 0.33061993 0.5154378  0.02350986]\n",
            "10 [0.42684034 0.96876353 0.9687421  0.91864324 0.652549   0.23873931\n",
            " 0.12476125 0.22765237 0.12635851 0.33061993 0.51543784 0.0235098 ]\n",
            "7 [0.35849398 0.96876395 0.88753533 0.8159371  0.95633745 0.5185603\n",
            " 0.82517636 0.68038833 0.5095168  0.02350992 0.02350992 0.02350992]\n",
            "1 [0.35849398 0.9687639  0.88753533 0.81593704 0.95633745 0.51856023\n",
            " 0.82517636 0.6803883  0.5095167  0.02350992 0.02350992 0.02350992]\n",
            "11 [0.35849398 0.9687639  0.88753533 0.81593704 0.95633745 0.51856023\n",
            " 0.82517636 0.6803883  0.5095167  0.02350992 0.02350992 0.02350992]\n",
            "1 [0.358494   0.96876395 0.88753533 0.8159371  0.95633745 0.51856023\n",
            " 0.8251764  0.68038833 0.5095168  0.02350989 0.02350989 0.02350989]\n",
            "11 [0.358494   0.96876395 0.88753533 0.8159371  0.95633745 0.51856023\n",
            " 0.8251764  0.68038833 0.5095168  0.02350989 0.02350989 0.02350989]\n",
            "2 [0.358494   0.96876395 0.88753533 0.8159371  0.9563375  0.5185603\n",
            " 0.8251764  0.68038833 0.5095168  0.02350992 0.02350992 0.02350992]\n",
            "11 [0.358494   0.96876395 0.88753533 0.8159371  0.9563375  0.5185603\n",
            " 0.8251764  0.68038833 0.5095168  0.02350992 0.02350992 0.02350992]\n",
            "2 [0.358494   0.96876395 0.8875354  0.8159371  0.9563375  0.5185603\n",
            " 0.82517636 0.68038833 0.5095168  0.02350995 0.02350995 0.02350995]\n",
            "8 [0.358494   0.96876395 0.8875354  0.8159371  0.9563375  0.5185603\n",
            " 0.82517636 0.68038833 0.5095168  0.02350995 0.02350995 0.02350995]\n",
            "2 [0.35849404 0.96876395 0.88753533 0.81593704 0.95633745 0.5185602\n",
            " 0.82517636 0.6803883  0.5095167  0.02350983 0.02350983 0.02350983]\n",
            "11 [0.35849404 0.96876395 0.88753533 0.81593704 0.95633745 0.5185602\n",
            " 0.82517636 0.6803883  0.5095167  0.02350983 0.02350983 0.02350983]\n",
            "2 [0.35849404 0.96876395 0.88753533 0.81593704 0.95633745 0.51856023\n",
            " 0.82517636 0.68038833 0.5095167  0.02350989 0.02350989 0.02350989]\n",
            "11 [0.35849404 0.96876395 0.88753533 0.81593704 0.95633745 0.51856023\n",
            " 0.82517636 0.68038833 0.5095167  0.02350989 0.02350989 0.02350989]\n",
            "8 [0.358494   0.96876395 0.88753533 0.8159371  0.9563375  0.5185603\n",
            " 0.82517636 0.68038833 0.5095168  0.02350995 0.02350995 0.02350995]\n",
            "11 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.5514729  0.7971402  0.92373836 0.1940473 ]\n",
            "1 [0.3611054  0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971401  0.9237383  0.1940473 ]\n",
            "11 [0.3611054  0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971401  0.9237383  0.1940473 ]\n",
            "2 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971402  0.92373836 0.1940473 ]\n",
            "11 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971402  0.92373836 0.1940473 ]\n",
            "2 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.5514729  0.7971402  0.92373836 0.19404733]\n",
            "11 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.5514729  0.7971402  0.92373836 0.19404733]\n",
            "2 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.55147284 0.7971402  0.92373836 0.1940473 ]\n",
            "11 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.55147284 0.7971402  0.92373836 0.1940473 ]\n",
            "2 [0.3611055  0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840515  0.8253728  0.5514728  0.7971402  0.92373836 0.19404733]\n",
            "11 [0.3611055  0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840515  0.8253728  0.5514728  0.7971402  0.92373836 0.19404733]\n",
            "2 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971401  0.92373836 0.1940473 ]\n",
            "11 [0.36110544 0.96876097 0.9687612  0.91172755 0.7589736  0.47364396\n",
            " 0.6840516  0.8253728  0.55147284 0.7971401  0.92373836 0.1940473 ]\n",
            "11 [0.36110544 0.96876097 0.96876115 0.91172755 0.7589736  0.473644\n",
            " 0.6840516  0.8253729  0.5514729  0.7971402  0.92373836 0.19404733]\n",
            "0.09 0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQnIByP_tObl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
